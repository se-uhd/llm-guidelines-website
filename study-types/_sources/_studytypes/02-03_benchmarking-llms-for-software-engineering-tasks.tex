\studytypesubsection{Benchmarking LLMs for Software Engineering Tasks (S7)}
\label{sec:benchmarking-llms-for-software-engineering-tasks}

\studytypeparagraph{Description}

Benchmarking is the process of evaluating an LLM's performance on standardized tasks, using standardized metrics, on a standardized dataset. The LLM output is compared to a supposed ground truth from the benchmark dataset. Typical tasks include code generation, code summarization, code completion, and code repair, but also natural language processing tasks, such as anaphora resolution (i.e., the task of identifying the referring expression of a word or phrase occurring earlier in the text). %, interesting for subfields such a Requirements Engineering. 
Metrics may include general metrics for text generation, such as \emph{ROUGE}, \emph{BLEU}, or \emph{METEOR}~\cite{10.1145/3695988}, or task-specific metrics, such as \emph{CodeBLEU} for code generation.  Benchmarking requires high-quality reference datasets.

\studytypeparagraph{Example(s)}

In SE, benchmarking may include evaluating an LLM's ability to produce accurate and reliable outputs for a given input (usually a task description, possibly accompanied by data obtained from curated real-world projects or from synthetic SE-specific datasets). \emph{RepairBench}~\cite{silva2024repairbench}, for example, contains 574 buggy Java methods and their corresponding fixed versions, which can be used to evaluate the performance of LLMs in code repair tasks. It uses the \emph{Plausible@1} metric (i.e., the probability that the first generated patch passes all test cases) and the \emph{AST Match@1} metric (i.e., the probability that the abstract syntax tree of the first generated patch matches the ground truth patch).
\emph{SWE-Bench}~\cite{DBLP:conf/iclr/JimenezYWYPPN24} is a more generic benchmark that contains 2,294 SE Python tasks extracted from GitHub pull requests.
To score the LLM's task performance, the benchmark validates whether the generated patch successfully compiles and calculates the percentage of passed test cases. Meanwhile, \emph{HumanEval}~\cite{DBLP:journals/corr/abs-2107-03374} is often used to assess code generation. 
