Using LLMs in empirical SE research, whether as tools for researchers or for software engineers, offers several potential advantages but also raises fundamental challenges that cut across the study types described above.

\studytypeparagraph{Advantages}

The primary advantage of using LLMs for research tasks is \emph{speed, cost reduction, and scalability}. LLMs can annotate, judge, synthesize, and simulate faster and at lower cost than human researchers, with studies showing cost reductions of 50--96\% on various natural language tasks~\cite{DBLP:conf/emnlp/WangLXZZ21} (e.g., \citeauthor{DBLP:conf/chi/HeHDRH24} found that GPT-4 annotation required only two days and 122.08 USD compared to several weeks and 4,508 USD for a comparable MTurk pipeline~\cite{DBLP:conf/chi/HeHDRH24}). Similarly, augmenting or replacing human participants with LLM-generated virtual participants would reduce recruitment effort~\cite{DBLP:conf/vl/Madampe0HO24}. This efficiency unlocks scalability: qualitative research traditionally does not scale well to large samples, but LLMs allow researchers to analyze larger datasets~\cite{DBLP:journals/ase/BanoHZT24, barros2024largelanguagemodelqualitative, le√ßa2024applicationsimplicationslargelanguage} and support larger judgment datasets.

LLMs can also \emph{automate} tasks such as coding qualitative data, assessing artifact quality, and generating summaries, reducing cognitive demands and resources required for qualitative research. Some studies suggest that LLMs may improve \emph{consistency}: ChatGPT's accuracy exceeded crowd workers by approximately 25\%, and LLMs can achieve higher inter-rater agreement than crowd workers and trained annotators~\cite{DBLP:journals/corr/abs-2303-15056}. However, no compelling evidence supports claims that LLMs are less biased than human annotators~\cite{DBLP:journals/corr/abs-2303-15056}.

LLMs could potentially provide \emph{access to otherwise-inaccessible research contexts}. \textit{If} virtual participants' behavior is sufficiently similar to human participants, LLMs could access underrepresented and hard-to-reach populations, strengthen generalizability and inclusiveness, impute missing data (see \synthesis), and enable research that is ethically problematic with real humans (e.g., questions that would force a human to relive past trauma do not harm an LLM).

Studying real-world LLM-based tools allows researchers to \emph{understand the state of practice}, uncovering usage patterns, adoption rates, and contextual factors, and \emph{generate hypotheses} about how LLMs affect developer productivity, collaboration, and decision-making. From an engineering perspective, developing LLM-based tools \emph{requires less task-specific engineering} than traditional SE approaches such as static analysis or symbolic execution, because a single model can handle diverse inputs without language-specific parsers or hand-crafted rules. Good benchmarks provide \emph{standardized evaluation and model comparison}, reducing research effort and fostering open science by providing common ground for sharing data and results. Benchmarks built for specific SE tasks can help identify LLM weaknesses and support optimization and fine-tuning.

\studytypeparagraph{Challenges}

The stochastic nature of LLM responses, where identical prompts may yield different outputs, \emph{undermines replicability} across all study types, complicating interpretation of experimental results and test-retest reliability. More broadly, \emph{reliability} is a persistent concern: conceptualizing an LLM-as-judge or LLM-as-annotator as a measurement instrument, researchers should expect validity, test-retest reliability, inter-rater reliability, minimal error, and measurement invariance~\cite{ralph2024teaching}. However, LLMs show significant variability depending on the dataset and task~\cite{DBLP:journals/corr/abs-2306-00176}, are sensitive to prompt variations~\cite{DBLP:journals/corr/abs-2304-11085} and option order~\cite{DBLP:conf/naacl/PezeshkpourH24}, can behave differently when reviewing their own outputs~\cite{NEURIPS2024_7f1f0218}, and can be unreliable for high-stakes labeling~\cite{DBLP:conf/chi/Wang0RMM24}. Crucially, \emph{reliability does not imply validity}: a reliable LLM might be reliably inaccurate~\cite{DBLP:conf/coling/ZhouC024}, and for tasks with no single correct answer, the statistical framework pushes outputs toward the most likely answer, which may not be the best one~\cite{DBLP:journals/corr/abs-2406-18403}. See \modelversion and \limitationsmitigations for reporting guidance on replicability and reliability.

The \emph{rapid evolution} of LLMs and LLM-based tools, combined with professionals rapidly learning how to employ them, complicates longitudinal comparisons and may quickly render study findings obsolete (e.g., GPT-4's accuracy on a simple mathematical task dropped from 84\% to 51\% within three months~\cite{DBLP:journals/corr/abs-2307-09009}).

The prevalence of \emph{proprietary tools and opaque training data} limits researchers' ability to assess and mitigate biases, and enables benchmark contamination~\cite{DBLP:journals/corr/abs-2410-16186}. LLMs exhibit \emph{bias} in multiple forms: tendencies to overestimate certain labels~\cite{DBLP:journals/corr/abs-2304-10145}, well-documented fairness issues~\cite{Gallegos2024BiasAF}, and the potential to reinforce prejudices. When simulating human participants, LLMs are \enq{likely to both misportray and flatten the representations of demographic groups}~\cite{DBLP:journals/corr/abs-2402-01908}. See \openllm and \limitationsmitigations for mitigation strategies.

\emph{Evaluation difficulty} is inherent in studying LLM-based tools and benchmarks. Benchmarks may lack construct validity~\cite{ralph2024teaching}, usually do not capture the full complexity of software engineering work~\cite{Chandra2025benchmarks}, and may lead to \emph{overconfidence} and overfitting~\cite{DBLP:conf/kdd/WanSJKCNSSWYABJ24}. LLMs are also susceptible to adversarial manipulation where semantics-preserving changes may deceive them into accepting flawed artifacts~\cite{DBLP:journals/corr/abs-2510-24367}. See \benchmarksmetrics for detailed guidance on benchmark design, including~\citeauthor{cao2025should}'s~\cite{cao2025should} guidelines for coding task benchmarks.

A fundamental challenge is \emph{philosophical and methodological incongruence} with qualitative research. In a recent open letter, 419 experienced qualitative researchers argued \enq{that analytic approaches such as reflexive thematic analysis are human research practices requiring a subjective, positioned, and reflexive researcher and therefore the use of GenAI in such approaches is not methodologically congruent}~\cite{jowsey2025reject}. LLMs lack the capacity for genuinely reflexive qualitative analysis because they operate on statistical prediction without understanding the meaning of the data being analyzed~\cite{jowsey2025reject, bano2023exploringqualitativeresearchusing, DBLP:journals/corr/abs-2510-18456}. Effective use requires structured prompts and careful human oversight~\cite{barros2024largelanguagemodelqualitative}; see \humanvalidation and \limitationsmitigations for guidance.

There is \emph{insufficient evidence} for the effectiveness of LLMs in most research roles. No compelling evidence exists that LLMs can accurately simulate human participants~\cite{schroeder2025llmspsychology} or reliably judge most relevant properties of SE artifacts; gathering such evidence for each specific usage may be quite difficult~\cite{DBLP:journals/ais/HardingDLL24}. LLMs may be better suited for augmenting rather than replacing human researchers~\cite{DBLP:conf/emnlp/WangLXZZ21, DBLP:conf/chi/HeHDRH24}, but even then, limited evidence exists that augmentation increases \emph{effectiveness} as well as \emph{efficiency}~\cite{schroeder2025llmspsychology}. When LLM outputs are incorrect, they can negatively \emph{affect human judgment}~\cite{DBLP:conf/www/HuangKA23a, panHumanCenteredDesignRecommendations2024}; see \humanvalidation for mitigation strategies.

Pooling multiple outputs for majority voting improves reliability but increases \emph{cost and environmental impact}~\cite{DBLP:journals/corr/abs-2304-11085, DBLP:conf/emnlp/WangLXZZ21}. While open models are available, the most capable ones require substantial hardware; relying on \emph{cloud-based APIs} introduces concerns related to data privacy, security, and replicability. See \limitationsmitigations for sustainability considerations.

Field study findings face \emph{generalizability} challenges because outcomes may be highly sensitive to individual differences, usage patterns, goals, and contexts. Field studies must be ``dependable''~\cite{Sullivan2011-ub} beyond traditional validity criteria, which complicates methodology; see \limitationsmitigations for a detailed threat taxonomy.
