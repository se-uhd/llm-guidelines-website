\input{../../header.tex}

\begin{document}

\subsection{Introduction: LLMs as Tools for Software Engineering Researchers}
\label{sec:llms-as-tools-for-software-engineering-researchers}

LLMs can serve as powerful tools to help researchers conduct empirical studies.
They can automate various tasks such as data collection, pre-processing, and analysis.
For example, LLMs can apply predefined coding guides to qualitative datasets (\annotators), assess the quality of software artifacts (\judges), generate summaries of research papers (\synthesis), or simulate human behavior in empirical studies (\subjects).
This can significantly reduce the time and effort required to conduct a study.
However, besides the many advantages that all these applications bring, they also come with challenges such as potential threats to validity and implications for the reproducibility of study results.

%\par\noindent\hdashrule{\columnwidth}{0.5pt}{2pt 2pt}

\subsection{LLMs as Annotators}
\label{sec:llms-as-annotators}

\subsubsection{Description}
Just like human annotators, LLMs can label artifacts based on a pre-defined coding guide.
However, they can label data much faster than any human could. 
In qualitative data analysis, manually annotating (``coding'') natural language text, e.g., in software artifacts, open-ended survey responses, or interview transcripts, is a time-consuming manual process~\cite{DBLP:journals/ase/BanoHZT24}.
LLMs can be used to augment or replace human annotations, provide suggestions for new codes (see Section \synthesis), or even automate the entire qualitative data analysis process.

\subsubsection{Example(s)}
Recent work in software engineering has begun to explore the use of LLMs for annotation tasks.
\citeauthor{Huang2023Enhancing}~\cite{Huang2023Enhancing} proposed an approach that utilizes multiple LLMs for joint annotation of mobile application reviews. 
They used three models of comparable size with an absolute majority voting rule (i.e., a label is only accepted if it receives more than half of the total votes from the models).
Accordingly, the annotations fell into three categories: exact matches (where all models agreed), partial matches (where a majority agreed), and non-matches (where no majority was reached).
The study by \citeauthor{DBLP:conf/msr/AhmedDTP25}~\cite{DBLP:conf/msr/AhmedDTP25} examined LLMs as annotators in software engineering research across five datasets, six LLMs, and ten annotation tasks.
They found that model-model agreement strongly correlates with human-model agreement, suggesting situations in which LLMs could effectively replace human annotators. Their research showed that for tasks where humans themselves disagreed significantly , models also performed poorly.
Conversely, if multiple LLMs reach similar solutions independently, then LLMs are likely suitable for the annotation task.
They proposed to use model confidence scores to identify specific samples that could be safely delegated to LLMs, potentially reducing human annotation effort without compromising inter-rater agreement.

\subsubsection{Advantages}
Recent research demonstrates several advantages of using LLMs as annotators, including their cost effectiveness and accuracy.
LLM-based annotation can dramatically reduce costs compared to human labeling, with studies showing cost reductions of 50-96\% on various natural language tasks~\cite{DBLP:conf/emnlp/WangLXZZ21}.
For example, \citeauthor{DBLP:conf/chi/HeHDRH24} found in their study that GPT-4 annotation costs only \$122.08 compared to \$4,508 for a comparable MTurk pipeline~\cite{DBLP:conf/chi/HeHDRH24}.
Moreover, the LLM-based approach resulted in a completion time of just 2 days versus several weeks for the crowd-sourced approach.
LLMs consistently demonstrate strong performance, with ChatGPT's accuracy exceeding crowd workers by approximately 25\% on average~\cite{DBLP:journals/corr/abs-2303-15056}, achieving impressive results in specific tasks such as sentiment analysis (65\% accuracy)~\cite{DBLP:journals/corr/abs-2304-10145}.
LLMs also show remarkably high inter-rater agreement, higher than crowd workers and trained annotators~\cite{DBLP:journals/corr/abs-2303-15056}.

\subsubsection{Challenges}
Challenges of using LLMs as annotators include reliability issues, human-LLM interaction challenges, biases, errors, and resource considerations.
Studies suggest that while LLMs show promise as annotation tools in SE research, their optimal use may be in augmenting rather than replacing human annotators~\cite{DBLP:conf/emnlp/WangLXZZ21, DBLP:conf/chi/HeHDRH24}.
LLMs can negatively affect human judgment when labels are incorrect~\cite{DBLP:conf/www/HuangKA23a}, and their overconfidence requires careful verification~\cite{DBLP:conf/kdd/WanSJKCNSSWYABJ24}.
Moreover, in previous studies, LLMs have shown significant variability in annotation quality depending on the dataset and the annotation task~\cite{DBLP:journals/corr/abs-2306-00176}.
Studies have shown that LLMs are especially unreliable for high-stakes labeling tasks~\cite{DBLP:conf/chi/Wang0RMM24} and that LLMs can have notable performance disparities between label categories~\cite{DBLP:journals/corr/abs-2304-10145}.
Recent empirical evidence indicates that LLM consistency in text annotation often falls below scientific reliability thresholds, with outputs being sensitive to minor prompt variations~\cite{DBLP:journals/corr/abs-2304-11085}.
Context-dependent annotations pose a specific challenge, as LLMs show difficulty in correctly interpreting text segments that require broader contextual understanding~\cite{DBLP:conf/chi/HeHDRH24}.
Although pooling of multiple outputs can improve reliability, this approach requires additional computational resources and still requires validation against human-annotated data.
While generally cost-effective, LLM annotation requires careful management of per token charges, particularly for longer texts~\cite{DBLP:conf/emnlp/WangLXZZ21}. Furthermore, achieving reliable annotations may require multiple runs of the same input to enable majority voting~\cite{DBLP:journals/corr/abs-2304-11085}, although the exact cost comparison between LLM-based and human annotation is controversial~\cite{DBLP:conf/chi/HeHDRH24}.
Finally, research has identified consistent biases in label assignment, including tendencies to overestimate certain labels and misclassify neutral content~\cite{DBLP:journals/corr/abs-2304-10145}.

%\par\noindent\hdashrule{\columnwidth}{0.5pt}{2pt 2pt}

\subsection{LLMs as Judges}
\label{sec:llms-as-judges}

\subsubsection{Description}

LLMs can act as judges or raters to evaluate properties of software artifacts.
For example, LLMs can be used to assess code readability, adherence to coding standards, or the quality of code comments.
Judgment is distinct from the more qualitative task of assigning a code or label to typically unstructured text (see Section \annotators).
It is also different from using LLMs for general software engineering tasks, as discussed in Section \llmsforresearcher.

\subsubsection{Example(s)}

\citeauthor{DBLP:conf/re/LubosFTGMEL24}~\cite{DBLP:conf/re/LubosFTGMEL24} leveraged \href{https://www.llama.com/llama2/}{Llama-2} to evaluate the quality of software requirements statements. 
They prompted the LLM with the text below, where the words in curly brackets reflect the study parameters:

\begin{quote}
\small
Your task is to evaluate the quality of a software requirement.\\
Evaluate whether the following requirement is \{quality\_characteristic\}. \\
\{quality\_ characteristic\} means: \{quality\_characteristic\_explanation\}\\
The evaluation result must be: `yes' or `no'.\\
Request: Based on the following description of the project:
\{project\_description\}\\
Evaluate the quality of the following requirement: \{requirement\}.\\
Explain your decision and suggest an improved version.\\
\end{quote}

\citeauthor{DBLP:conf/re/LubosFTGMEL24} evaluated the LLM's output against human judges, to assess how well the LLM matched experts. 
Agreement can be measured in many ways; this study used Cohen's kappa~\cite{cohen60} and found moderate agreement for simple requirements and poor agreement for more complex requirements.
However, human evaluation of machine judgments may not be the same as evaluation of human judgments and is an open area of research~\cite{DBLP:journals/corr/abs-2410-03775}. 
A crucial decision in studies focusing on LLMs as judges is the number of examples provided to the LLM.
This might involve no tuning (zero-shot), several examples (few-shot), or providing many examples (closer to traditional training data).
In the example above, \citeauthor{DBLP:conf/re/LubosFTGMEL24} chose zero-shot tuning, providing no specific guidance besides the provided context; they did not show the LLM what a `yes' or `no' answer might look like. 
In contrast, \citeauthor{wang2025multimodalrequirementsdatabasedacceptance}~\cite{wang2025multimodalrequirementsdatabasedacceptance} provided a rubric to an LLM when treating LLMs as a judge to produce interpretable scales (0 to 4) for the overall quality of acceptance criteria generated from user stories.
They combined this with a lower-level reward stage to refine acceptance criteria, which significantly improved correctness, clarity, and alignment with the user stories.

\subsubsection{Advantages}

Depending on the model configuration, LLMs can provide relatively consistent evaluations.
They can further help mitigate human biases and the general variability that human judges might introduce.
This may lead to more reliable and reproducible results in empirical studies, to the extent that these models can be reproduced or checkpointed.
LLMs can be much more efficient and scale more easily than the equivalent human approach.
With LLM automation, entire datasets can be assessed, as opposed to subsets, and the assessment produced can be at the selected level of granularity, e.g., binary outputs (`yes' or `no') or defined levels (e.g., 0 to 4).
However, the main constraint that varies by model and budget is the input context size, i.e., the number of tokens one can pass into a model.
For example, the upper bound of the context that can be passed to OpenAi's \textsf{o1-mini} model is \href{https://help.openai.com/en/articles/9855712-openai-o1-models-faq-chatgpt-enterprise-and-edu}{32k tokens}.

\subsubsection{Challenges}

When relying on the judgment of LLMs, researchers must build a \textit{reliable} process for generating judgment labels that considers the non-deterministic nature of LLMs and report the intricacies of that process transparently~\cite{DBLP:journals/corr/abs-2412-12509}.
For example, the order of options has been shown to affect LLM outputs in multiple-choice settings~\cite{DBLP:conf/naacl/PezeshkpourH24}. 
In addition to reliability, other quality attributes to consider include the \textit{accuracy} of the labels. % and the speed and \textit{scalability} of the process. 
For example, a reliable LLM might be reliably inaccurate and wrong. 
Evaluating and judging large numbers of items, for example, to perform fault localization on the thousands of bugs that large open-source projects have to deal with, comes with costs in clock time, compute time, and environmental sustainability.
Evidence shows that LLMs can behave differently when reviewing their own outputs~\cite{NEURIPS2024_7f1f0218}.
In more human-oriented datasets (such as discussions of pull requests) LLMs may suffer from well-documented biases and issues with fairness~\cite{Gallegos2024BiasAF}. 
For tasks in which human judges disagree significantly, it is not clear if an LLM judge should reflect the majority opinion or act as an independent judge.
The underlying statistical framework of an LLM usually pushes outputs towards the most likely (majority) answer. 
There is ongoing research on how suitable LLMs are as independent judges.
Questions about bias, accuracy, and trust remain~\cite{DBLP:journals/corr/abs-2406-18403}.
There is reason for concern about LLMs judging student assignments or doing peer review of scientific papers~\cite{DBLP:conf/coling/ZhouC024}.
Even beyond questions of technical capacity, ethical questions remain, particularly if there is some implicit expectation that a human is judging the output.
Involving a human in the judgment loop, for example, to contextualize the scoring, is one approach~\cite{panHumanCenteredDesignRecommendations2024}. 
However, the lack of large-scale ground truth datasets for benchmarking LLM performance in judgment studies hinders progress in this area.

%\par\noindent\hdashrule{\columnwidth}{0.5pt}{2pt 2pt}

\subsection{LLMs for Synthesis}
\label{sec:llms-for-synthesis}

\subsubsection{Description}

LLMs can support synthesis tasks in software engineering research by processing and distilling information from qualitative data sources.
In this context, synthesis refers to the process of integrating and interpreting information from multiple sources to generate higher-level insights, identify patterns across datasets, and develop conceptual frameworks or theories.
Unlike annotation (see Section \annotators), which focuses on categorizing or labeling individual data points, synthesis involves connecting and interpreting these annotations to develop a cohesive understanding of the phenomenon being studied.
Synthesis tasks can also mean generating synthetic datasets (e.g., source code, bug-fix pairs, requirements, etc.) that are then used in downstream tasks to train, fine-tune, or evaluate existing models or tools.
In this case, the synthesis is done primarily using the LLM and its training data; the input is limited to basic instructions and examples.

\subsubsection{Example(s)} 

Published examples of applying LLMs for synthesis in the software engineering domain are still scarce.
However, recent work has explored the use of LLMs for qualitative synthesis in other domains, allowing reflection on how LLMs can be applied for this purpose in software engineering~\cite{DBLP:journals/ase/BanoHZT24}.
\citeauthor{barros2024largelanguagemodelqualitative}~\cite{barros2024largelanguagemodelqualitative} conducted a systematic mapping study on the use of LLMs for qualitative research.
They identified examples in domains such as healthcare and social sciences (see, e.g., \cite{de2024performing,mathis2024inductive}) in which LLMs were used to support qualitative analysis, including grounded theory and thematic analysis.
Overall, the findings highlight the successful generation of preliminary coding schemes from interview transcripts, later refined by human researchers, along with support for pattern identification.
This approach was reported not only to expedite the initial coding process but also to allow researchers to focus more on higher-level analysis and interpretation.
However, \citeauthor{barros2024largelanguagemodelqualitative} emphasize that effective use of LLMs requires structured prompts and careful human oversight.
%This particular paper suggests using LLMs to support tasks such as initial coding and theme identification while conservatively reserving interpretative or creative processes for human analysts.
Similarly, \citeauthor{leça2024applicationsimplicationslargelanguage}~\cite{leça2024applicationsimplicationslargelanguage} conducted a systematic mapping study to investigate how LLMs are used in qualitative analysis and how they can be applied in software engineering research.
Consistent with the study by \citeauthor{barros2024largelanguagemodelqualitative}~\cite{barros2024largelanguagemodelqualitative}, \citeauthor{leça2024applicationsimplicationslargelanguage} identified that LLMs are applied primarily in tasks such as coding, thematic analysis, and data categorization, reducing the time, cognitive demands, and resources required for these processes.
Finally, \citeauthor{DBLP:journals/corr/abs-2506-21138}'s work is an example of using LLMs to create synthetic datasets.
They present an approach to generate synthetic requirements, showing that they \enq{can match or surpass human-authored requirements for specific classification tasks}~\cite{DBLP:journals/corr/abs-2506-21138}.

\subsubsection{Advantages}

LLMs offer promising support for synthesis in SE research by helping researchers process artifacts such as interview transcripts and survey responses, or by assisting literature reviews.
Qualitative research in SE traditionally faces challenges such as limited scalability, inconsistencies in coding, difficulties in generalizing findings from small or context-specific samples, and the influence of the researchers' subjectivity on data interpretation~\cite{DBLP:journals/ase/BanoHZT24}. 
The use of LLMs for synthesis can offer advantages in addressing these challenges~\cite{DBLP:journals/ase/BanoHZT24, barros2024largelanguagemodelqualitative, leça2024applicationsimplicationslargelanguage}.
LLMs can reduce manual effort and subjectivity, improve consistency and generalizability, and assist researchers in deriving codes and developing coding guides during the early stages of qualitative data analysis~\cite{DBLP:conf/chi/ByunVS23,DBLP:journals/ase/BanoHZT24}.
LLMs can enable researchers to analyze larger datasets, identifying patterns across broader contexts than traditional qualitative methods typically allow. In addition, they can help mitigate the effects of human subjectivity.
However, while LLMs streamline many aspects of qualitative synthesis, careful oversight remains essential.
% Models can develop coding guides and identify emerging themes, potentially automating the entire synthesis process across large qualitative datasets.

\subsubsection{Challenges}

Although LLMs have the potential to automate synthesis, concerns about overreliance remain, especially due to discrepancies between AI- and human-generated insights, particularly in capturing contextual nuances~\cite{bano2023exploringqualitativeresearchusing}.
\citeauthor{bano2023exploringqualitativeresearchusing}~\cite{bano2023exploringqualitativeresearchusing} found that while LLMs can provide structured summaries and qualitative coding frameworks, they may misinterpret nuanced qualitative data due to a lack of contextual understanding.
Other studies have echoed similar concerns~\cite{DBLP:journals/ase/BanoHZT24, barros2024largelanguagemodelqualitative, leça2024applicationsimplicationslargelanguage}.
In particular, LLMs cannot independently assess the validity of arguments.
Critical thinking remains a human responsibility in qualitative synthesis.
\citeauthor{peterschineyee2025generalizationbias} have shown that popular LLMs and LLM-based tools tend to overgeneralize results when summarizing scientific articles, that is, they \enq{produce broader generalizations of scientific results than those in the original.}
Compared to human-authored summaries, \enq{LLM summaries were nearly five times more likely to contain broad generalizations}~\cite{peterschineyee2025generalizationbias}.
In addition, LLMs can produce biased results, reinforcing existing prejudices or omitting essential perspectives, making human oversight crucial to ensure accurate interpretation, mitigate biases, and maintain quality control.
Moreover, the proprietary nature of many LLMs limits transparency.
In particular, it is unknown how the training data might affect the synthesis process.
Furthermore, reproducibility issues persist due to the influence of model versions and prompt variations on the synthesis result.

%\par\noindent\hdashrule{\columnwidth}{0.5pt}{2pt 2pt}

% Reverted back to "subject", as participant seems to be primarily used for humans: see  https://dictionary.apa.org/subject and https://dictionary.apa.org/participant
\subsection{LLMs as Subjects}
\label{sec:llms-as-subjects}

\subsubsection{Description}

In empirical studies, data is collected from participants through methods such as surveys, interviews, or controlled experiments.
LLMs can serve as \emph{subjects} in empirical studies by simulating human behavior and interactions (we use the term ``subject'' since ``participant'' implies being human~\cite{apa-dict-subject}).
In this capacity, LLMs generate responses that approximate those of human participants, which makes them particularly valuable for research involving user interactions, collaborative coding environments, and software usability assessments.
This approach enables data collection that closely reflects human reactions while avoiding the need for direct human involvement.
To achieve this, prompt engineering techniques are widely employed, with a common approach being the use of the \textit{Personas Pattern}~\cite{DBLP:journals/corr/abs-2308-07702}, which involves tailoring LLM responses to align with predefined profiles or roles that emulate specific user archetypes.
\citeauthor{ZHAO2025101167} outline opportunities and challenges of using LLMs as research subjects in detail~\cite{ZHAO2025101167}.
Furthermore, recent sociological studies have emphasized that, to be effectively utilized in this capacity, LLMs---including their agentic versions---should meet four criteria of algorithmic fidelity~\cite{DBLP:journals/corr/abs-2209-06899}.
Generated responses should be: (1) indistinguishable from human-produced texts (e.g., LLM-generated code reviews should be comparable to those from real developers); (2) consistent with the attitudes and sociodemographic information of the conditioning context (e.g., LLMs simulating junior developers should exhibit different confidence levels, vocabulary, and concerns compared to senior engineers); (3) naturally aligned with the form, tone, and content of the provided context (e.g., responses in an agile stand-up meeting simulation should be concise, task-focused, and aligned with sprint objectives rather than long, formal explanations); and (4) reflective of patterns in relationships between ideas, demographics, and behavior observed in comparable human data (e.g., discussions on software architecture decisions should capture trade-offs typically debated by human developers, such as maintainability versus performance, rather than abstract theoretical arguments).

\subsubsection{Example(s)}

LLMs can be used as subjects in various types of empirical studies, enabling researchers to simulate human participants.
The broader applicability of LLM-based studies beyond software engineering has been compiled by \citeauthor{DBLP:journals/ipm/XuSRGPLSH24}~\cite{DBLP:journals/ipm/XuSRGPLSH24}, who examined various uses in social science research.
Given the socio-technical nature of software development, some of these approaches are transferable to empirical software engineering research.
For example, LLMs can be applied in survey and interview studies to impersonate developers responding to survey questionnaires or interviews, allowing researchers to test the clarity and effectiveness of survey items or to simulate responses under varying conditions, such as different levels of expertise or cultural contexts.
For example, \citeauthor{DBLP:journals/ase/GerosaTSS24}~\cite{DBLP:journals/ase/GerosaTSS24} explored persona-based interviews and multi-persona focus groups, demonstrating how LLMs can emulate human responses and behaviors while addressing ethical concerns, biases, and methodological challenges.
Another example are usability studies, in which LLMs can simulate end-user feedback, providing insights into potential usability issues and offering suggestions for improvement based on predefined user personas. This aligns with the work of \citeauthor{bano2025doessoftwareengineerlook}~\cite{bano2025doessoftwareengineerlook}, who investigated biases in LLM-generated candidate profiles in SE recruitment processes.
Their study, which analyzed both textual and visual inputs, revealed biases favoring male, Caucasian candidates, lighter skin tones, and slim physiques, particularly for senior roles.

\subsubsection{Advantages}

Using LLMs as subjects can reduce the effort of recruiting human participants, a process that is often time-consuming and costly~\cite{DBLP:conf/vl/Madampe0HO24}, by augmenting existing datasets or, in some cases, completely replacing human participants. 
In interviews and survey studies, LLMs can simulate diverse respondent profiles, enabling access to underrepresented populations, which can strengthen the generalizability of findings. 
In data mining studies, LLMs can generate synthetic data (see \synthesis) to fill gaps where real-world data is unavailable or underrepresented.

\subsubsection{Challenges}

It is important that researchers are aware of the inherent biases~\cite{Crowell2023} and limitations~\cite{DBLP:journals/ais/HardingDLL24, DBLP:journals/corr/abs-2402-01908} when using LLMs as study subjects. 
\citeauthor{schroeder2025llmspsychology}~\cite{schroeder2025llmspsychology}, who have studied discrepancies between LLM and human responses, even conclude that \enq{LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.}
One critical concern is construct validity.
LLMs have been shown to misrepresent demographic group perspectives, failing to capture the diversity of opinions and experiences within a group.
The use of identity-based prompts can reduce identities to fixed and innate characteristics, amplifying perceived differences between groups.
These biases introduce the risk that studies which rely on LLM-generated responses may inadvertently reinforce stereotypes or misrepresent real-world social dynamics.
Alternatives to demographic prompts can be employed when the goal is to broaden response coverage~\cite{DBLP:journals/corr/abs-2402-01908}.
%To mitigate these issues, encoded identity names, that is, statistically representative person names associated with specific demographic groups, can be used instead of explicit labels (e.g., using ``Mario Rossi'' instead of the label ``Italian white man'').
%Moreover, the temperature setting can be increased to enhance response diversity.
%All three strategies aim to reduce the risk of stereotyped, flattened, or overly deterministic outputs by introducing indirectness or variability into the prompt, thereby encouraging the model to generate responses that better reflect the internal diversity and nuance of human perspectives.
Beyond construct validity, internal validity must also be considered, particularly with regard to causal conclusions based on studies relying on LLM-simulated responses.
Finally, external validity remains a challenge, as findings based on LLMs may not generalize to humans.


\subsection{Introduction: LLMs as Tools for Software Engineers}
\label{sec:llms-as-tools-for-software-engineers}

LLM-based assistants have become an essential tool for software engineers, supporting them in various tasks such as code generation and debugging.
Researchers have studied how software engineers use LLMs (\llmusage), developed new tools that integrate LLMs (\newtools), and benchmarked LLMs for software engineering tasks (\benchmarkingtasks).
%Like in the previous section, we outline the advantages that studying LLMs in these contexts brings, but also point to specific challenges.

%\par\noindent\hdashrule{\columnwidth}{0.5pt}{2pt 2pt}

\subsection{Studying LLM Usage in Software Engineering}
\label{sec:studying-llm-usage-in-software-engineering}

\subsubsection{Description}

Studying how software engineers use LLMs is crucial to understand the current state of practice in SE.
Researchers can observe software engineers' usage of LLM-based tools in the field, or study if and how they adopt such tools, their usage patterns, as well as perceived benefits and challenges.
Surveys, interviews, observational studies, or analysis of usage logs can provide insights into how LLMs are integrated into development processes, how they influence decision making, and what factors affect their acceptance and effectiveness. 
Such studies can inform improvements for existing LLM-based tools, motivate the design of novel tools, or derive best practices for LLM-assisted software engineering.
They can also uncover risks or deficiencies of existing tools.

\subsubsection{Example(s)}

\citeauthor{DBLP:journals/pacmse/KhojahM0N24} investigated the use of ChatGPT (GPT-3.5) by professional software engineers in a week-long observational study~\cite{DBLP:journals/pacmse/KhojahM0N24}.
They found that most developers do not use the code generated by ChatGPT directly but instead use the output as a guide to implement their own solutions.
%Furthermore, they recommend that future research investigate the use of ChatGPT in non code-related SE tasks and for training and learning SE concepts.
\citeauthor{DBLP:conf/csee/AzanzaPIG24} conducted a case study that evaluated the impact of introducing LLMs on the onboarding process of new software developers~\cite{DBLP:conf/csee/AzanzaPIG24} (GPT-3).
Their study identified potential in the use of LLMs for onboarding, as it can allow newcomers to seek information on their own, without the need to \enq{bother} senior colleagues.
%However, they also highlight significant privacy concerns with the use of proprietary, third-party LLMs.
Surveys can help researchers to quickly provide an overview of the current perceptions of LLM usage.
For example, \citeauthor{DBLP:conf/icsa/JahicS24} surveyed participants from 15 software companies regarding their practices on LLMs in software engineering~\cite{DBLP:conf/icsa/JahicS24}.
They found that the majority of study participants had already adopted AI for software engineering tasks; most of them used ChatGPT.
Multiple participants cited copyright and privacy issues, as well as inconsistent or low-quality outputs, as barriers to adoption.
Retrospective studies that analyze data generated while developers use LLMs can provide additional insights into human-LLM interactions.
For example, researchers can employ data mining methods to build large-scale conversation datasets, such as the DevGPT dataset introduced by \citeauthor{DBLP:conf/msr/XiaoTHM24}~\cite{DBLP:conf/msr/XiaoTHM24}.
Conversations can then be analyzed using quantitative~\cite{DBLP:conf/msr/RabbiCZI24} and qualitative~\cite{DBLP:conf/msr/MohamedPP24} analysis methods.

\subsubsection{Advantages}

Studying the real-world usage of LLM-based tools allows researchers to understand the state of practice and guide future research directions.
In field studies, researchers can uncover usage patterns, adoption rates, and the influence of contextual factors on usage behavior.
Outside of a controlled study environment, researchers can uncover contextual information about LLM-assisted SE workflows beyond the specific LLMs being evaluated.
This may, for example, help researchers generate hypotheses about how LLMs impact developer productivity, collaboration, and decision-making processes.
In controlled laboratory studies, researchers can study specific phenomena related to LLM usage under carefully regulated, but potentially artificial conditions.
Specifically, they can isolate individual tasks in the software engineering workflow and investigate how LLM-based tools may support task completion.
Furthermore, controlled experiments allow for direct comparisons between different LLM-based tools.
The results can then be used to validate general hypotheses about human-LLM interactions in SE.

\subsubsection{Challenges}

When conducting field studies in real-world environments, researchers have to ensure that their study results are \enq{dependable}~\cite{Sullivan2011-ub} beyond the traditional validity criteria such as internal or construct validity.
The usage environment in a real-world context can often be extremely diverse.
The integration of LLMs can range from specific LLMs based on company policy to the unregulated use of any available LLM. 
Both extremes may influence the adoption of LLM by software engineers, and hence need to be addressed in the study methodology.
In addition, in longitudinal case studies, the timing of the study may have a significant impact on its result, as LLMs and LLM-based tools are rapidly evolving.
Moreover, developers are still learning how to best make use of the new technology, and best practices are still being developed and established.
Furthermore, the predominance of proprietary commercial LLM-based tools in the market poses a significant barrier to research.
Limited access to telemetry data or other usage metrics restricts the ability of researchers to conduct comprehensive analyses of real-world tool usage.
To make study results reliable, researchers must establish that their results are rigorous, for example, by using triangulation to understand and minimize potential biases~\cite{Sullivan2011-ub}.
When it comes to studying LLM usage in controlled laboratory studies, researchers may struggle with the inherent variability of LLM outputs.
Since reproducibility and precision are essential for hypothesis testing, the stochastic nature of LLM responses---where identical prompts may yield different outputs across participants---can complicate the interpretation of experimental results and affect the reliability of study findings.

%\par\noindent\hdashrule{\columnwidth}{0.5pt}{2pt 2pt}

\subsection{LLMs for New Software Engineering Tools}
\label{sec:llms-for-new-software-engineering-tools}

\subsubsection{Description}

LLMs are being integrated into new tools that support software engineers in their daily tasks, e.g., to assist in code comprehension~\cite{DBLP:conf/chi/YanHWH24} and test case generation~\cite{DBLP:journals/tse/SchaferNET24}.
One way of integrating LLM-based tools into software engineers' workflows are GenAI agents.
Unlike traditional LLM-based tools, these agents are capable of acting autonomously and proactively, are often tailored to meet specific user needs, and can interact with external environments~\cite{takerngsaksiri2024human,wiesinger2025agents}.
From an architectural perspective, GenAI agents can be implemented in various ways~\cite{wiesinger2025agents}.
However, they generally share three key components: (1) a reasoning mechanism that guides the LLM (often enabled by advanced prompt engineering), (2) a set of tools to interact with external systems (e.g., APIs or databases), and (3) a user communication interface that extends beyond traditional chat-based interactions~\cite{DBLP:conf/icsm/RichardsW24, DBLP:journals/tmlr/SumersYN024, DBLP:journals/corr/abs-2309-07870}.
Researchers can also test and compare different tool architectures to increase artifact quality and developer satisfaction.

\subsubsection{Example(s)}

\citeauthor{DBLP:conf/chi/YanHWH24} proposed IVIE, a tool integrated into the VS Code graphical interface that generates and explains code using LLMs~\cite{DBLP:conf/chi/YanHWH24}.
The authors focused more on the presentation, providing a user-friendly interface to interact with the LLM. 
\citeauthor{DBLP:journals/tse/SchaferNET24}~\cite{DBLP:journals/tse/SchaferNET24} presented a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation.
They presented TestPilot, a tool that implements an approach in which the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from the documentation.
\citeauthor{DBLP:conf/icsm/RichardsW24} introduced a preliminary GenAI agent designed to assist developers in understanding source code by incorporating a reasoning component grounded in the theory of mind~\cite{DBLP:conf/icsm/RichardsW24}.

\subsubsection{Advantages}

From an engineering perspective, developing LLM-based tools is easier than implementing many traditional SE approaches such as static analysis or symbolic execution.
Depending on the capabilities of the underlying model, it is also easier to build tools that are independent of a specific programming language.
This enables researchers to build tools for a more diverse set of tasks.
In addition, it allows them to test their tools in a wider range of contexts.

\subsubsection{Challenges}

Traditional approaches, such as static analysis, are deterministic. LLMs are not.
Although the non-determinism of LLMs can be mitigated using configuration parameters and prompting strategies, this poses a major challenge.
It can be challenging for researchers to evaluate the effectiveness of a tool, as minor changes in the input can lead to major differences in the performance.
Since the exact training data is often not published by model vendors, a reliable assessment of tool performance for unknown data difficult.
From an engineering perspective, while open models are available, the most capable ones require substantial hardware resources.
Using cloud-based APIs or relying on third-party providers for hosting, while seemingly a potential solution, introduces new concerns related to data privacy and security.

%\par\noindent\hdashrule{\columnwidth}{0.5pt}{2pt 2pt}

\subsection{Benchmarking LLMs for Software Engineering Tasks}
\label{sec:benchmarking-llms-for-software-engineering-tasks}

\subsubsection{Description}

Benchmarking is the process of evaluating an LLM's performance using standardized tasks and metrics, which requires high-quality reference datasets.
LLM output is compared to a ground truth from the benchmark dataset using general metrics for text generation, such as \emph{ROUGE}, \emph{BLEU}, or \emph{METEOR}~\cite{10.1145/3695988}, or task-specific metrics, such as \emph{CodeBLEU} for code generation.
For example, \emph{HumanEval}~\cite{DBLP:journals/corr/abs-2107-03374} is often used to assess code generation, establishing it as a de facto standard.

\subsubsection{Example(s)}

In SE, benchmarking may include the evaluation of an LLM's ability to produce accurate and reliable outputs for a given input, usually a task description, which may be accompanied by data obtained from curated real-world projects or from synthetic SE-specific datasets.
Typical tasks include code generation, code summarization, code completion, and code repair, but also natural language processing tasks, such as anaphora resolution (i.e., the task of identifying the referring expression of a word or phrase occurring earlier in the text). %, interesting for subfields such a Requirements Engineering. 
\emph{RepairBench}~\cite{silva2024repairbench}, for example, contains 574 buggy Java methods and their corresponding fixed versions, which can be used to evaluate the performance of LLMs in code repair tasks.
This benchmark uses the \emph{Plausible@1} metric (i.e., the probability that the first generated patch passes all test cases) and the \emph{AST Match@1} metric (i.e., the probability that the abstract syntax tree of the first generated patch matches the ground truth patch).
\emph{SWE-Bench}~\cite{DBLP:conf/iclr/JimenezYWYPPN24} is a more generic benchmark that contains 2,294 SE Python tasks extracted from GitHub pull requests.
To score the LLM's performance on the tasks, the benchmark validates whether the generated patch is applicable (i.e., successfully compiles) and calculates the percentage of passed test cases.

\subsubsection{Advantages}

Properly built benchmarks provide objective, reproducible evaluation across different tasks, enabling a comparison between different models (and versions).
In addition, benchmarks built for specific SE tasks can help identify LLM weaknesses and support their optimization and fine-tuning for such tasks.
They can foster open science practices by providing a common ground for sharing data (e.g., as part of the benchmark itself) and results (e.g., of models run against a benchmark).
Benchmarks built using real-world data can help legitimize research results for practitioners, supporting industry-academia collaboration.
However, benchmarks are subject to several challenges.

\subsubsection{Challenges}

Benchmark contamination, that is, the inclusion of the benchmark in the LLM training data~\cite{DBLP:journals/corr/abs-2410-16186}, has recently been identified as a problem.
The careful selection of samples and the creation of the corresponding input prompts is particularly important, as correlations between prompts may bias the benchmark results~\cite{DBLP:conf/acl/SiskaMAB24}.
Although LLMs might perform well on a specific benchmark such as \emph{HumanEval}, they do not necessarily perform well on other benchmarks.
Moreover, benchmark metrics such as \emph{perplexity} or \emph{BLEU-N do} not neccessarily reflect human judgment.
Recently, \citeauthor{cao2025should}~\cite{cao2025should} has proposed guidelines for the creation of LLM benchmarks related to coding tasks, grounded in a systematic survey of existing benchmarks. 
In this process, they highlight current shortcomings related to reliability, transparency, irreproducibility, low data quality, and inadequate validation measures.
For more details on benchmarks, see Section \benchmarksmetrics. 

\subsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}
