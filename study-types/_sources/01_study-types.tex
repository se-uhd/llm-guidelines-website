\input{../../header.tex}

\begin{document}

\subsection{Overview}

The development of empirical guidelines for software engineering (SE) studies involving large language models (LLMs) is crucial for ensuring the validity and reproducibility of results.
However, these guidelines must be tailored for different study types that pose unique challenges.
Therefore, understanding the classification of studies involving LLMs is essential for developing appropriate guidelines.
Note that we currently focus on large language models, that is, on natural language processing.
In the future, we might extend and generalize our focus to include multimodal foundation models.

Our \guidelines use the following study types to contextualize the recommendations we provide.
We present the study types independent of specific guidelines, that is, the guidelines refer to the study types but not the other way around.

\subsection{Introduction: LLMs as Tools for Software Engineering Researchers}

LLMs can be leveraged as powerful tools to assist researchers conducting empirical studies.
They can automate various tasks such as data collection, preprocessing, and analysis.
For example, LLMs can apply pre-defined coding guides to large qualitative datasets (\annotators), assess the quality of software artifacts (\judges), generate summaries of research papers (\synthesis), and even simulate human behavior in empirical studies (\participants).
This can significantly reduce the time and effort required by researchers, allowing them to focus on other aspects of their studies.
However, besides the many \textbf{advantages} that all these applications bring, they also come with \textbf{challenges} such as potential threats to validity and implications for the reproducibility of study results.


\subsection{LLMs as Annotators}

\subsubsection{Description}

Just like human annotators, LLMs can label artifacts based on a pre-defined coding guide.
However, they can label data much faster than any human could. 
In qualitative data analysis, manually annotating (``coding'') natural language text, e.g. in software artifacts, open-ended survey responses, or interview transcripts, is a time-consuming manual process~\cite{DBLP:journals/ase/BanoHZT24}.
LLMs can be used to augment or even replace human annotations, provide suggestions for new codes (see Section \synthesis), or even automate the entire process.

\subsubsection{Example(s)}

Recent work in software engineering has begun exploring the use of LLMs for annotation tasks.
Huang et al.~\cite{Huang2023Enhancing} proposed an approach leveraging multiple LLMs for joint annotation of mobile application reviews. 
They used three models of comparable size with an absolute majority voting rule (i.e., a label is only accepted if it receives more than half of the total votes from the models).
Accordingly, the annotations fell into three categories: exact matches (where all models agreed), partial matches (where a majority agreed), and non-matches (where no majority was reached).

The study by Ahmed et al.~\cite{Ahmed2025} examined LLMs-as-annotators in software engineering research across five datasets and ten annotation tasks. Using six state-of-the-art LLMs to perform tasks previously done by humans, they found that model-model agreement strongly correlates with human-model agreement, suggesting situations in which LLMs could effectively replace human annotators. Their research showed that for tasks where humans themselves disagree significantly (such as evaluating code conciseness), models also perform poorly. Conversely, if multiple LLMs reach similar solutions independently, then LLMs are likely suitable for the annotation task. They proposed using model confidence scores to identify specific samples that could be safely delegated to LLMs, potentially reducing human annotation effort without compromising inter-rater agreement.

\subsubsection{Advantages}

Recent research demonstrates several key advantages of using LLMs as annotators, in particular their cost-effectiveness as well as efficiency and accuracy benefits.
LLM-based annotation dramatically reduces costs compared to human labeling, with studies showing cost reductions of 50-96\% across various natural language tasks~\cite{DBLP:conf/emnlp/WangLXZZ21}.
For example, He et al. found in their study that GPT-4 annotation cost only \$122.08 compared to \$4,508 for a comparable MTurk pipeline.
Moreover, the LLM-based approach resulting in a completion time of just 2 days versus several weeks for the crowdsourced approach~\cite{DBLP:conf/chi/HeHDRH24}.
Moreover, LLMs consistently demonstrate strong performance, with ChatGPT's accuracy exceeding crowd workers by approximately 25\% on average~\cite{DBLP:journals/corr/abs-2303-15056} and achieving impressive results in specific tasks such as sentiment analysis (65\% accuracy) and counterspeech detection (79\% precision)~\cite{DBLP:journals/corr/abs-2304-10145}. They also show remarkably high inter-coder agreement, surpassing both crowd workers and trained annotators~\cite{DBLP:journals/corr/abs-2303-15056}.

\subsubsection{Challenges}

Challenges of using LLMs as annotators include reliability issues, human-LLM interaction challenges, biases, errors, and resource considerations.
Studies suggest that while LLMs show promise as annotation tools in SE research, their optimal use may be in augmenting rather than replacing human annotators~\cite{DBLP:conf/emnlp/WangLXZZ21, DBLP:conf/chi/HeHDRH24}.
LLMs can negatively impact human judgment when their labels are incorrect~\cite{DBLP:conf/www/HuangKA23a}, and their overconfidence requires careful verification~\cite{DBLP:conf/kdd/WanSJKCNSSWYABJ24}.
Moreover, in previous studies, LLMs have shown significant variability in annotation quality based on the dataset and the annotation task~\cite{DBLP:journals/corr/abs-2306-00176}. Studies have shown LLMs to be especially unreliable for high-stakes labeling tasks~\cite{DBLP:conf/chi/Wang0RMM24} and have demonstrated that LLMs can have notable performance disparities across label categories~\cite{DBLP:journals/corr/abs-2304-10145}. Recent empirical evidence indicates that LLM consistency in text annotation often falls below scientific reliability thresholds, with outputs being sensitive to minor prompt variations~\cite{DBLP:journals/corr/abs-2304-11085}.
Context-dependent annotations pose a specific challenge, as LLMs show difficulty in correctly interpreting text segments that require broader contextual understanding~\cite{DBLP:conf/chi/HeHDRH24}.
While pooling multiple outputs can improve reliability, this approach necessitates additional computational resources and still requires validation against human-annotated data.
While generally cost-effective, LLM annotation requires careful management of per-token charges, particularly for longer texts~\cite{DBLP:conf/emnlp/WangLXZZ21}. Furthermore, achieving reliable annotations may require multiple runs of the same input to enable majority voting~\cite{DBLP:journals/corr/abs-2304-11085}, although exact cost comparisons between LLM-based and human annotation is controversial~\cite{DBLP:conf/chi/HeHDRH24}.
Finally, research has identified consistent biases in label assignment, including tendencies to overestimate certain labels and misclassify neutral content, particularly in stance detection tasks~\cite{DBLP:journals/corr/abs-2304-10145}.


\subsection{LLMs as Judges}

\subsubsection{Description}

LLMs can act as judges or raters to evaluate properties of software artifacts.
For instance, LLMs can be used to assess code readability, adherence to coding standards, or the quality of code comments.
Judgment is distinct from the more qualitative task of assigning a code or label to, typically unstructured, text (see Section \annotators).
It is also distinct from using LLMs for general software engineering tasks, as we discuss in Section \llmsforresearcher.

\subsubsection{Example(s)}

Lubos et al.~\cite{DBLP:conf/re/LubosFTGMEL24} leveraged \href{https://www.llama.com/llama2/}{Llama-2} to evaluate the quality of software requirements statements. 
They prompted the LLM with the text below, where the words in brackets reflect parameters for the study:

\begin{quote}
Your task is to evaluate the quality of a software requirement.\\
Evaluate whether the following requirement is \{quality\_characteristic\}. \\
\{quality\_ characteristic\} means: \{quality\_characteristic\_explanation\}\\
The evaluation result must be: `yes' or `no'.\\
Request: Based on the following description of the project:
\{project\_description\}\\
Evaluate the quality of the following requirement: \{requirement\}.\\
Explain your decision and suggest an improved version.\\
\end{quote}

The authors then evaluated the LLM's output against human raters, to assess how well the LLM matched experts. 
Agreement can be measured in many ways; this study used Cohen's kappa measure~\cite{cohen60} and found moderate agreement for simple requirements, and poor agreement for more complex requirements.
However, evaluating machine judgment may not be the same as human-human judgments, and is an open area of research~\cite{DBLP:journals/corr/abs-2410-03775}. 

One crucial decision in such studies is the number of examples provided to the LLM.
This might involve no tuning (zero-shot), several examples (few-shot), or providing many examples (closer to traditional training data).
In the example above, Lubos et al. chose zero-shot tuning, providing no specific guidance besides the project's context, i.e., they did not show the LLM what a yes or no answer might look like. 

\subsubsection{Advantages}

By providing, depending on the model configuration, relatively consistent and evaluations, LLMs can help mitigate human biases and the variability that human judges might introduce.
This may lead to more reliable and reproducible results in empirical studies, to the extent these models can be reproduced or checkpointed.
LLMs can be much more efficient, and scale far more easily, than the equivalent human approach. With LLM automation, entire datasets could be assessed, as opposed to subsets. The main constraint, which varies by model and budget, is the input context size, i.e., the number of tokens one can pass into a model. For example, the upper bound on context that can be passed into OpenAi's \textsf{o1-mini} model is \href{https://help.openai.com/en/articles/9855712-openai-o1-models-faq-chatgpt-enterprise-and-edu}{32k tokens}. 

\subsubsection{Challenges}

When relying on the judgment of LLMs, researchers must build a \textit{reliable} process for generating judgment labels that considers the non-deterministic nature of LLMs and report the intricacies of that process transparently~\cite{DBLP:journals/corr/abs-2412-12509}. For example, the order of options has been shown to affect LLM outputs in multiple-choice settings~\cite{DBLP:conf/naacl/PezeshkpourH24}. 
In addition to reliability, other quality attributes include \textit{accuracy} of the labels, and the speed and \textit{scalability} of the LLM tool. 
A reliable LLM might be reliably inaccurate and wrong. 
Evaluating and judging large numbers of items---for example, to perform fault localization on the thousands of bugs big open-source projects deal with---comes with costs in clock time, compute time, and environmental sustainability.

Evidence shows LLMs can behave differently, for instance, when reviewing their own outputs~\cite{NEURIPS2024_7f1f0218}. In more human-oriented datasets (such as discussions of pull requests) LLMs may suffer from well documented biases and issues with fairness~\cite{Gallegos2024BiasAF}. 
For tasks where human judges themselves disagree significantly, it is not clear if an LLM judge should reflect the majority opinion or act as an independent judge. The underlying statistical framework of an LLM usually pushes outputs towards the most likely (majority) answer. 

Research is ongoing as to how suitable LLMs are as standalone judges. Questions around bias, accuracy, and trust remain~\cite{DBLP:journals/corr/abs-2406-18403}. There is reason for concern about LLMs judging student assignments or doing peer review of scientific papers~\cite{DBLP:conf/coling/ZhouC024}. Even beyond questions of technical capacity, ethical questions remain, particularly if there is some implicit expectation that a human is judging the output, such as in a quiz. Involving a human in the judgment loop---for example, to contextualize the scoring---is one approach~\cite{panHumanCenteredDesignRecommendations2024}. 
A lack of large-scale ground truth datasets for benchmarking LLM performance on judgment studies is hindering progress on evaluating research in this area.


\subsection{LLMs for Synthesis}

\subsubsection{Description}

LLMs can support synthesis tasks in software engineering research by processing and distilling information from qualitative data sources.
In this context, synthesis refers to the process of integrating and interpreting information from multiple sources to generate higher-level insights, identify patterns across datasets, and develop conceptual frameworks or theories. Unlike annotation (see Section \annotators), which focuses on categorizing or labeling individual data points, synthesis involves connecting and interpreting these annotations to develop a cohesive understanding of the phenomenon being studied.
Synthesis tasks can also include generating synthetic SE datasets (e.g., source code, bug-fix pairs, requirements, etc.) that are used in downstream tasks to train, fine-tune, or evaluate existing models or tools.
In this case, the synthesis is done primarily based on the training data of the model, the input that researchers provide is limited to prompts and examples.

\subsubsection{Example(s)} 

While published examples of applying LLMs for synthesis in the software engineering domain are still scarce, recent work has explored the use of LLMs for qualitative synthesis in other domains, enabling a reflection on how LLMs can be applied for this purpose in software engineering~\cite{DBLP:journals/ase/BanoHZT24}. Barros \textit{et al.}~\cite{barros2024largelanguagemodelqualitative} conducted a systematic mapping study on the use of LLMs for qualitative research. They identified examples in domains such as healthcare and social sciences (see, e.g., \cite{de2024performing,mathis2024inductive}) in which LLMs were used to support different methodologies for qualitative analysis, including grounded theory and thematic analysis. Overall, the findings highlight the successful generation of preliminary coding schemes from interview transcripts, later refined by human researchers, along with support for pattern identification. This approach was reported not only to expedite the initial coding process but also to allow researchers to focus more on higher-level analysis and interpretation. However, they emphasize that effective use of LLMs requires structured prompts and careful human oversight. This particular paper suggests using LLMs to support tasks such as initial coding and theme identification while conservatively reserving interpretative or creative processes for human analysts.
Similarly, Lecça \textit{et al.}~\cite{leça2024applicationsimplicationslargelanguage} conducted a systematic mapping study to investigate how LLMs are used in qualitative analysis and how they can be applied in software engineering research. Consistent with the study by Barros \textit{et al.}~\cite{barros2024largelanguagemodelqualitative}, they identified that LLMs are applied primarily in tasks like coding, thematic analysis, and data categorization, providing efficiency by reducing the time, cognitive demands, and resources often required for these processes.

\subsubsection{Advantages}

LLMs offer promising support for synthesis in SE research by helping to process artifacts such as interview transcripts, survey responses, and literature reviews.
Qualitative research in software engineering traditionally faces several challenges, including limited scalability due to the manual nature of the analysis, inconsistencies in coding across researchers, difficulties in generalizing findings from small or context-specific samples, and the influence of researcher subjectivity on data interpretation~\cite{DBLP:journals/ase/BanoHZT24}. 
The use of LLMs for synthesis can offer advantages in addressing these classical challenges~\cite{DBLP:journals/ase/BanoHZT24, barros2024largelanguagemodelqualitative, leça2024applicationsimplicationslargelanguage}.
LLMs can reduce manual effort and subjectivity, improve consistency and generalizability, and assist researchers in deriving codes and developing coding guides during the early stages of qualitative data analysis~\cite{DBLP:conf/chi/ByunVS23,DBLP:journals/ase/BanoHZT24}. The codes generated by LLMs can then be used to annotate additional data, with the models identifying emerging themes and generating candidate insights, potentially automating the entire synthesis process across large qualitative datasets.
LLMs can enable researchers to analyze larger datasets, identifying patterns across broader contexts than traditional qualitative methods typically allow. Additionally, they can help mitigate the effects of human subjectivity. Nevertheless, while LLMs streamline many aspects of qualitative synthesis, careful oversight remains essential to ensure nuanced interpretation and contextual accuracy.

\subsubsection{Challenges}

While LLMs have the potential to automate synthesis, concerns about overreliance remain, especially due to discrepancies between AI- and human-generated insights, particularly in capturing contextual nuances~\cite{bano2023exploringqualitativeresearchusing}.
Bano \textit{et al.}~\cite{bano2023exploringqualitativeresearchusing} found that while LLMs can provide structured summaries and qualitative coding frameworks, they may misinterpret nuanced qualitative data due to their lack of contextual understanding. Other studies have echoed similar concerns~\cite{DBLP:journals/ase/BanoHZT24, barros2024largelanguagemodelqualitative, leça2024applicationsimplicationslargelanguage}. In particular, LLMs cannot independently assess argument validity, and critical thinking remains a human responsibility in qualitative synthesis.
Peters and Chin-Yee have shown that popular LLMs and LLM-based tools tend to overgeneralize results when summarizing scientific articles, that is they \enq{produce broader generalizations of scientific results than those in the original.} Compared to human-authored summaries, \enq{LLM summaries were nearly five times more likely to contain broad generalizations}~\cite{peterschineyee2025generalizationbias}.

Moreover, LLMs may produce biased results, reinforcing existing prejudices or omitting key perspectives, making human oversight essential to ensure accurate interpretation, mitigate biases, and maintain quality control.
The proprietary nature of many LLMs limits transparency. In particular, it is unknown how the inclusion of certain training data might affect the synthesis process.
Furthermore, reproducibility issues persist due to inconsistencies across inferences, model versions, and prompt variations.


% Reverted back to "subject", as participant seems to be primarily used for humans: see  https://dictionary.apa.org/subject and https://dictionary.apa.org/participant
\subsection{LLMs as Subjects}

\subsubsection{Description}

In empirical studies, data is collected from participants through methods such as surveys, interviews, or controlled experiments.
LLMs can serve as \emph{subjects} in empirical studies by simulating human behavior and interactions.
We use the term ``subject'' since ``participant'' implies being human.\footnote{\url{https://dictionary.apa.org/subject}}
In this capacity, LLMs generate responses that approximate those of human participants, making them particularly valuable for research involving user interactions, collaborative coding environments, and software usability assessments.
This approach enables data collection that closely reflects human reactions while avoiding the need for direct human involvement.
To achieve this, prompt engineering techniques are widely employed, with a common approach being the use of the \textit{Personas Pattern}~\cite{DBLP:journals/corr/abs-2308-07702}, which involves tailoring LLM responses to align with predefined profiles or roles that emulate specific user archetypes.
Zhao et al. outline opportunities and challenges of using LLMs as research subjects in detail~\cite{ZHAO2025101167}.
Furthermore, recent sociological studies have emphasized that, to be effectively utilized in this capacity, LLMs---including their agentic versions tailored through prompt engineering---should meet four criteria of algorithmic fidelity~\cite{DBLP:journals/corr/abs-2209-06899}; generated responses should be: indistinguishable from human-produced texts (e.g., LLM-generated code reviews should be comparable to those from real developers); consistent with the attitudes and sociodemographic information of the conditioning context (e.g., LLMs simulating junior developers should exhibit different confidence levels, vocabulary, and concerns compared to senior engineers); naturally aligned with the form, tone, and content of the provided context (e.g., responses in an agile stand-up meeting simulation should be concise, task-focused, and aligned with sprint objectives rather than long, formal explanations); and reflective of patterns in relationships between ideas, demographics, and behavior observed in comparable human data (e.g., discussions on software architecture decisions should capture trade-offs typically debated by human developers, such as maintainability versus performance, rather than abstract theoretical arguments).

\subsubsection{Example(s)}

LLMs can be used as subjects in various types of empirical studies, enabling researchers to simulate human participants.
The broader applicability of LLM-based studies beyond software engineering has been compiled by Xu et al.~\cite{DBLP:journals/ipm/XuSRGPLSH24}, who examined various uses in social science research.
Given the socio-technical nature of software development, some of these approaches should be transferable to empirical software engineering research.

For example, LLMs can be applied in survey and interview studies to impersonate developers responding to survey questionnaires or interviews, allowing researchers to test the clarity and effectiveness of survey items or to simulate responses under varying conditions, such as different expertise levels or cultural contexts. For instance, Gerosa et al.~\cite{DBLP:journals/ase/GerosaTSS24} explored persona-based interviews and multi-persona focus groups, demonstrating how LLMs can emulate human responses and behaviors while addressing ethical concerns, biases, and methodological challenges.
Another example are usability studies, in which LLMs can simulate end-user feedback, providing insights into potential usability issues and offering suggestions for improvement based on predefined user personas. This aligns with the work of Bano et al.~\cite{bano2025doessoftwareengineerlook}, who investigated biases in LLM-generated candidate profiles in software engineering recruitment processes. Their study, which analyzed both textual and visual inputs, revealed biases favoring male, Caucasian candidates, lighter skin tones, and slim physiques, particularly for senior roles.

\subsubsection{Advantages}

Using LLMs as subjects offers valuable insights while significantly reducing the need to recruit human participants—a process that is often time-consuming and costly~\cite{DBLP:conf/vl/Madampe0HO24}—by augmenting existing datasets or, in some cases, even replacing human participants. 
Their use has potential applications across a wide range of empirical research methods.
In interviews and survey studies, they can simulate diverse respondent profiles, thus enhancing scalability and enabling access to underrepresented populations, which can strengthen the generalizability of findings. 
In data mining studies, LLMs can generate synthetic data to fill gaps where real-world data is unavailable or underrepresented.

\subsubsection{Challenges}

However, it is important that researchers are aware of LLMs' inherent biases~\cite{Crowell2023} and limitations~\cite{DBLP:journals/ais/HardingDLL24, DBLP:journals/corr/abs-2402-01908} when using them as study subjects. 
Schröder et al.~\cite{schroeder2025llmspsychology}, who have studied discrepancies between LLM and human responses, even conclude that \enq{LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.}
One critical concern is construct validity. LLMs have been shown to misrepresent demographic group perspectives, failing to capture the diversity of opinions and experiences within a group.
The use of identity-based prompts can reduce identities to fixed and innate characteristics, amplifying perceived differences between groups.
These biases introduce a risk that studies relying on LLM-generated responses may inadvertently reinforce stereotypes or misrepresent real-world social dynamics.
To mitigate these issues, encoded identity names---statistically representative person names associated with specific demographic groups---can be used instead of explicit labels (e.g., using ``Mario Rossi'' instead of the label ``Italian white man''), the temperature setting can be increased to enhance response diversity, and alternatives to demographic prompts can be employed when the goal is to broaden response coverage~\cite{DBLP:journals/corr/abs-2402-01908}.
All three strategies aim to reduce the risk of stereotyped, flattened, or overly deterministic outputs by introducing indirectness or variability into the prompt, thereby encouraging the model to generate responses that better reflect the internal diversity and nuance of human perspectives.
Beyond construct validity, internal validity must also be considered, particularly regarding causal conclusions based on studies relying on LLM-simulated responses.
External validity also remains a challenge, as findings based on LLMs may not generalize to humans.


\subsection{Introduction: LLMs as Tools for Software Engineers}

LLM-based assistants have become an essential tool for software engineers, supporting them in various tasks such as code generation, summarization, and repair.
Researchers have studied how software engineers use LLMs (\llmusage), developed new tools that integrate LLMs (\newtools), and benchmarked LLMs for software engineering tasks  (\benchmarkingtasks).
Like in the previous section, we outline the \textbf{advantages} that studying LLMs in these contexts brings, but also point to specific \textbf{challenges}.


\subsection{Studying LLM Usage in Software Engineering}

\subsubsection{Description}

Studying how software engineers use LLMs in their workflows is crucial for understanding the current state of practice in software engineering.
Researchers can observe software engineers' usage of LLM-based tools in the field, or study if and how they adopt such tools, their usage patterns, as well as perceived benefits and challenges.
Surveys, interviews, observational studies, or the analysis of usage logs, can provide insights into how LLMs are integrated into development processes, how they influence decision-making, and what factors affect their acceptance and effectiveness. 
Such studies can inform improvements for existing LLM-based tools, motivate the design of novel tools, or derive best practices for LLM-assisted software engineering.
However, they can also uncover risks or deficiencies of the tools.

\subsubsection{Example(s)}

Khojah et al. investigated the use of ChatGPT by professional software engineers in a week-long observational study~\cite{DBLP:journals/pacmse/KhojahM0N24} (GPT-3.5, data collection between March and July 2023).
They found that most developers do not use code generated by ChatGPT directly, but instead used the outputs as guidance to implement their own solutions.
Furthermore, they recommend that future research investigate the use of ChatGPT in non code-related SE tasks and for training and learning SE concepts.
Ananza et al. conducted a case study evaluating the impact of introducing LLMs to the onboarding process of new software developers~\cite{DBLP:conf/csee/AzanzaPIG24} (GPT-3.5, published 2024).
Their study identified potential in the use of LLMs for onboarding, as it may allow newcomers to seek information on their own, without the need to \enq{bother} senior colleagues.
However, they also highlight significant privacy concerns with the use of proprietary, third-party LLMs.
Surveys can help researchers quickly provide a wider overview of the current perceptions of LLM use.
For example, Jahic and Sami surveyed participants from 15 software companies regarding their practices on LLMs in software engineering~\cite{DBLP:conf/icsa/JahicS24} (published 2024).
They found that the majority of study participants had already adopted AI for software engineering tasks, with most of them using ChatGPT.
Multiple participants cited copyright and privacy issues, as well as the inconsistent or low quality outputs as barriers for adoption.
Retrospective studies analyzing the data generated during the use of LLMs by software engineers can provide additional insights into human LLM interactions.
For example, researchers can employ data mining methods to build large-scale conversation datasets, such as the DevGPT dataset introduced by Xiao et al~\cite{DBLP:conf/msr/XiaoTHM24} (data collection started in August 2023).
Conversations can then be analyzed using quantitative~\cite{DBLP:conf/msr/RabbiCZI24} and qualitative~\cite{DBLP:conf/msr/MohamedPP24} analysis methods (both studies use the DevGPT dataset).

\subsubsection{Advantages}

Studying the real-world usage of LLM-based tools allows researchers to understand the state of practice and guide future research directions.
In field studies, researchers may uncover usage patterns, adoption rates, and influence of contextual factors on the usage behavior.
Outside of controlled study environment, researchers may uncover nuanced information about LLM-assisted SE workflows beyond the specific LLMs being evaluated.
This may, for example, help researchers in generating hypotheses about how LLMs impact developer productivity, collaboration, and decision-making processes.

In controlled laboratory studies, researchers can study specific phenomena related to LLM usage under carefully regulated, but potentially artificial, conditions.
Specifically, they may isolate individual tasks in the software engineering workflow and investigate how LLM-based tools may support the task completion.
Furthermore, controlled experiments allow for direct comparisons between different LLM-based tools.
Results from these experiments can then be used validate general hypotheses about human LLM interactions in software engineering.

\subsubsection{Challenges}

When conducting field studies in real-world environments, researchers have to ensure that their study results are ``dependable''~\cite{Sullivan2011-ub} beyond the traditional validity criteria such as internal or construct validity.
The usage environment and choice of LLMs in a real-world context can often be extremely diverse.
In practice, the process integration of LLMs can range from the use of specific LLMs based on company policy to the unregulated use of any available LLM. 
Both extremes may influence the use of LLMs by software engineers in different ways, and as such should be addressed differently by the study methodology.
Further, in longitudinal case studies, the timing of the study may have a significant impact on its result, as LLMs are quickly being developed and replaced by newer versions.
This difficulty is exacerbated by the relative novelty of LLMs in the SE process.
Developers are still learning how to best make use of the new technology and best practices are being established.
Additionally, the predominance of proprietary commercial LLM-based tools in the market poses a significant barrier to research.
Limited access to telemetry data or other usage metrics restricts the ability of researchers to conduct comprehensive analyses of real-world tool utilization.
To nevertheless make study results dependable, researchers need to establish that their results are trustworthy and rigorous, e.g., by using triangulation, to understand and minimize potential biases~\cite{Sullivan2011-ub}.
 
When it comes to studying LLM usage in controlled laboratory studies, researchers may struggle with the inherent variability of LLM outputs.
Since reproducibility and precision are essential for hypothesis testing, the stochastic nature of LLM responses---where identical prompts may yield different outputs across participants---can complicate the interpretation of experimental results and affect the reliability of study findings.


\subsection{LLMs for New Software Engineering Tools}

\subsubsection{Description}

LLMs are being integrated into new tools supporting software engineers in their daily tasks, e.g., code comprehension~\cite{DBLP:conf/chi/YanHWH24} and test cases generation~\cite{DBLP:journals/tse/SchaferNET24}.
Such integration is important to tailor the tools to the specific needs of a development team, enhance their capabilities, and influence their behavior in accordance with users' needs.
An evolution of such integration are GenAI Agents.
Unlike simple applications powered by LLMs, these agents are capable of acting autonomously and proactively, are often tailored to meet specific user needs, and can interact with external environments~\cite{takerngsaksiri2024human,wiesinger2025agents}.
From an architectural perspective, GenAI Agents can be implemented in various ways~\cite{wiesinger2025agents}. However, they generally share three key components: a reasoning mechanism that guides the LLM (often enabled by advanced prompt engineering), a set of tools for interfacing with external systems (e.g., APIs or databases), and a user communication interface that extends beyond traditional chat-based interactions~\cite{DBLP:conf/icsm/RichardsW24, DBLP:journals/tmlr/SumersYN024, DBLP:journals/corr/abs-2309-07870}.
Other than propose tools, researcher can also evaluate the effectiveness of these tools in improving developer experience (for example, in terms of productivity, artefact quality, and developer satisfaction) and testing with different implementations of the above mentioned architecture in order to improve the tools.

\subsubsection{Example(s)}

As an example of LLMs-powered tool, Yan et al. proposed IVIE, a tool integrated into the VS Code graphical interface that generates and explains code using LLMs~\cite{DBLP:conf/chi/YanHWH24}. In this case, the authors focused more on the presentation part of the architecture, providing a user-friendly interface to interact with the LLM. 
Furthermore, Sch{\"{a}}fer et al.~\cite{DBLP:journals/tse/SchaferNET24} presented a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation. Concretely, they presented TestPilot: a tool implementing an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation.
More on the agentic side, Richards and Wessel introduced a preliminary GenAI agent designed to assist developers in understanding source code by incorporating a reasoning component grounded in the theory of mind~\cite{DBLP:conf/icsm/RichardsW24}. Their work specifically emphasized the reasoning aspect of the architecture, leveraging the theory of mind to develop tailored agents while also demonstrating their effectiveness.

\subsubsection{Advantages}

From an engineering perspective, developing LLM-based tools is easier than implementing many traditional SE approaches such as static analysis or symbolic execution.
Depending on the capabilities of the underlying model, it is also easier to build tools that are independent of a specific programming language.
This enables researchers to build tools for a more diverse set of tasks.
It furthermore allows them to test their tools in a wider range of contexts.

\subsubsection{Challenges}

Traditional approaches such as static analysis are deterministic. LLMs are not.
Although the non-determinism of LLMs can be mitigated using parameters and prompting strategies, this poses a major challenge.
It can be challenging for researchers to evaluate the effectiveness of their tool, as minor changes in the input can lead to major differences in the performance.
When using commercial LLMs, researchers face challenges because the training data is unknown, hindering a reliable assessment of tool performance for unknown data.
From an engineering perspective, while open models are available, the most performant ones require substantial hardware resources.
Resorting to cloud-based APIs or relying on third-party providers for hosting, while seemingly a potential solution, introduces new concerns related to data privacy and security.


\subsection{Benchmarking LLMs for Software Engineering Tasks}

\subsubsection{Description}

Benchmarking is the process of evaluating the LLM output for standardized tasks using standardized metrics.
Hence, high-quality reference datasets are necessary to perform evaluation across studies.
For example, HumanEval~\cite{DBLP:journals/corr/abs-2107-03374} is increasingly being used for the code generation, establishing it as de-facto standard for this task.   
LLM output is compared against a ground truth in the benchmark dataset using general metrics for text generation, such as ROUGE, BLEU, or METEOR~\cite{10.1145/3695988}, or task-specific metrics, such as CodeBLEU for code generation.

\subsubsection{Example(s)}

In software engineering, benchmarking may include the evaluation of an LLM's ability to produce accurate and reliable outputs for a given input, usually a task description which may be accompanied by data obtained from curated real-world projects or from synthetic SE-specific datasets (e.g., for few-shot prompting).
Typical tasks include code generation, code summarization, code completion, and code repair, but also natural-language processing tasks, such as anaphora resolution (i.e., the task of identifying the referring expression of a word or phrase occurring earlier in the text), interesting for subfields such a Requirements Engineering. 

RepairBench~\cite{silva2024repairbench}, for example, contains 574 buggy Java methods and their corresponding fixed versions, which can be used to evaluate the performance of LLMs in code repair tasks.
The metrics used are Plausible@1 (i.e., the probability that the first generated patch passes all test cases) and AST Match@1 (i.e., the probability that the abstract syntax tree of the first generated patch matches the one of the ground truth patch).
SWE-Bench~\cite{DBLP:conf/iclr/JimenezYWYPPN24} is a more generic benchmark that contains 2,294 SE Python tasks extracted from GitHub pull requests.
For scoring the performance of the LLMs on the tasks, the authors report whether the generated patch is applicable (i.e., it fails compilation) and, for successful patches, the percentage of test cases passed.

\subsubsection{Advantages}

Properly-built benchmarks provide objective, reproducible evaluation across different tasks, enabling a comparison between different models (and versions).
Moreover, benchmarks built for specific SE tasks can help identify LLM weaknesses and support their optimization/fine-tuning for such tasks.
Benchmark built using real-world data can also help legitimize research results for practitioners, supporting industry-academia collaboration.
Benchmarks are, however, subject to the issues outlined below.
Finally, benchmarks can foster open science practices by providing a common ground for sharing data (e.g., as part of the benchmark itself) and results (e.g., of models run against a benchmark).

\subsubsection{Challenges}

Benchmark contamination, the inclusion of the benchmark in the training dataset of the LLM,~\cite{DBLP:journals/corr/abs-2410-16186} has recently been identified as an issue.
The careful selection of samples and building of corresponding input prompts is particularly important, as correlations between prompts may bias benchmark results~\cite{DBLP:conf/acl/SiskaMAB24}.
While LLMs might perform well on a specific benchmark, such as HumanEval, it does not necessarily perform well on another benchmark.
Benchmark metrics such as perplexity or BLEU-N do not always reflect human judgment.
Recently, Cao et al.~\cite{cao2025should} has proposed guidelines for building benchmarks for LLMs related to coding tasks, grounded in a systematic survey of existing benchmarks. 
In this process, they highlight current shortcomings related to reliability, transparency irreproducibility, low data quality, and inadequate validation measures.
\comment{Previous comment not addressed yet: I like the last reference to guildelines for building a benchmark. Should we include some challenges from the paper, too?}

\subsection{References}

\bibliographystyle{plain}
\bibliography{../../literature.bib}

\end{document}
