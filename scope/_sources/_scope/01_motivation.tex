Since the release of ChatGPT in November 2022, large language models (LLMs) have rapidly become a major focus of software engineering (SE) research~\cite{10.1145/3695988}, yet the reproducibility and replicability of empirical studies involving LLMs remains uncertain.
Recent findings indicate a low reproducibility of SE studies involving LLMs~\cite{DBLP:journals/corr/abs-2510-25506}.
Three characteristics of LLMs make this particularly challenging:
their inherent non-determinism causes variability across runs, with slight changes leading to significantly different results~\cite{DBLP:journals/corr/abs-2412-12509, DBLP:conf/naacl/SongWLL25, DBLP:conf/nips/AgarwalSCCB21, bjarnason2026randomnessagenticevals};
commercial models evolve beyond version identifiers, so reported performance can change over time~\cite{DBLP:journals/corr/abs-2307-09009};
and even for ``open'' models, training data and fine-tuning details often remain undisclosed~\cite{Gibney2024}.
Moreover, prompt formatting choices alone can shift accuracy by tens of percentage points~\cite{DBLP:conf/iclr/Sclar0TS24}, and configured parameters such as temperature affect output variability~\cite{renze2024temperature}.
Hence, not reporting these settings directly affects reproducibility.

Although the SE research community has developed guidelines for conducting and reporting specific types of empirical studies such as controlled experiments (e.g.,~\cite{DBLP:books/sp/WohlinRHORW24, DBLP:books/sp/08/SSS2008}), their replications (e.g.,~\cite{DBLP:journals/tse/SantosVOJ21}), or empirical studies in general (e.g., the \emph{ACM SIGSOFT Empirical Standards}~\cite{ralph2021empiricalstandardssoftwareengineering}), none of these address the LLM-specific aspects described above.
Previously, a position paper highlighted these issues~\cite{DBLP:conf/wsese/0001BFB25}, but there was no comprehensive community-developed guidance for designing and reporting empirical studies involving LLMs in SE.

Therefore, we present community-developed guidelines for conducting and reporting studies involving LLMs in SE research, co-developed by 22 researchers.
After outlining our \scope, we introduce a taxonomy of \studytypes, then present eight \guidelines.
We complement these with an applicability matrix mapping guidelines to study types and a reporting checklist for authors and reviewers.
For each study type and guideline, we identify relevant examples, both within and outside of SE research.
We maintain the guidelines online as a living resource for the community to use and shape.\footnote{\url{https://llm-guidelines.org}}
