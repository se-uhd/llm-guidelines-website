\input{../../header.tex}

\begin{document}

\subsection{Report Prompts, their Development, and Interaction Logs}


\subsubsection{Recommendations}

\begin{quote}
\underline{tl;dr:} Researchers \must report all prompts in the \paper or \supplementarymaterial, including their structure, content, formatting, and dynamic components. Prompt development strategies (e.g., zero-shot, few-shot), rationale, and selection process \must be described. When prompts are long or complex, input handling and token optimization strategies \must be documented. For dynamically generated or user-authored prompts, generation and collection processes \must be reported. Prompt reuse across models and configurations \must be specified. If full prompt disclosure is not feasible, for example due to privacy or confidentiality concerns, summaries or examples \should be provided. Researchers \should report prompt revisions and pilot testing insights. To address model non-determinism and ensure reproducibility, especially when targeting SaaS-based commercial tools, full interaction logs (prompts + responses) \should be included.
\end{quote}

Prompts are critical for any study involving LLMs.
Depending on the task, prompts may include various \href{https://www.promptingguide.ai/introduction/elements}{types of content}, including instructions, context, and input data.
For software engineering tasks, the context can include source code, execution traces, error messages, and other forms of natural language content.
Prompts significantly influence a model’s output, and understanding how exactly they were formatted and integrated in an LLM-based study is essential for transparency, verifiability, and reproducibility.

Researchers \must report the complete prompts that were used, including all instructions, context, and input data.
The only exception to this is if transcripts might identify anonymous participants or reveal personal or confidential information, for example when working with industry partners.
This can be done with a structured template that contains placeholders for dynamically added content (see below).
Specifying the exact format is crucial.
For example, when using code snippets, researchers should specify whether they were enclosed in markdown-style code blocks (indented by four space characters, enclosed in triple backticks, etc.), if line breaks and whitespace were preserved, and whether additional annotations (e.g., comments) were included.
Similarly, for other artifacts such as error messages, stack traces, researchers should explain how these were presented.

Researchers \must also explain in the \paper how they developed the prompts and why they decided to follow certain prompting strategies.
If prompts from early phases of the research are unavailable, researchers \must at least summarize the prompt evolution.
However, given that prompts can be stored as plain text, there are established SE techniques such as version control systems that can used to collected the required provenance information.

They \must specify whether zero-shot, one-shot, or few-shot prompting was used. For few-shot prompts, the examples provided to the model should be clearly outlined, along with the rationale for selecting them.
If multiple versions of a prompt were tested, researchers should describe how these variations were evaluated and how the final design was chosen.

When dealing with extensive or complex prompts, such as those involving large codebases or multiple error logs, researchers \must describe the strategies they used for handling input length constraints.
Approaches might include truncating, summarizing, or splitting prompts into multiple parts.
Token optimization measures, such as simplifying code formatting or removing unnecessary comments, \must also be documented if applied.

In cases where prompts are generated dynamically—such as through preprocessing, template structures, or retrieval-augmented generation (RAG)—the process \must be thoroughly documented.
This includes explaining any automated algorithms or rules that influenced prompt generation.
For studies involving human participants, where users might create or modify prompts themselves, researchers \must describe how these prompts were collected and analyzed.
If full disclosure is not feasible due to privacy or confidentiality concerns, summaries and representative examples \should be provided.

To ensure full reproducibility, researchers \must make all prompts and prompt variations publicly available as part of their \supplementarymaterial.
If the full set of prompts is too extensive to include in the paper itself, researchers \should still provide representative examples and describe variations in the main body of the paper.
Since prompt effectiveness varies across models and model versions, researchers \must make clear which prompts were used for which models in which versions and with which configuration (see Section~\modelversion).
%For example: \textit{This prompt performed differently with GPT-4 (effective) versus Llama 2 (less effective) despite identical parameters.}

Prompt development is often iterative, involving collaboration between human researchers and AI tools.
Researchers \should report any instances where LLMs were used to suggest prompt refinements, as well as how those suggestions were incorporated.
Furthermore, prompts may need to be revised in response to failure cases where the model produced incorrect or incomplete outputs.
Iterative changes based on human feedback and pilot testing results \should also be included in the documentation.
A prompt changelog can help track and report the evolution of prompts throughout a research project, including key revisions, reasons for changes, and versioning (e.g., v1.0: initial prompt; v1.2: added output formatting; v2.0: incorporated examples of ideal responses).
Pilot testing and prompt evaluation are vital for ensuring that prompts yield reliable results. If such testing was conducted, researchers \should summarize key insights, including how different prompt variations affected output quality and which criteria were used to finalize the prompt design.

When trying to verify results, even with the exact same prompts, decoding strategies, and parameters, LLMs can still behave non-deterministically.
Non-determinism can arise from batching, input preprocessing, and floating point arithmetic on GPUs~\cite{Chann2023}.
Thus, in order to enable other researchers to verify the conclusions that researchers have drawn from LLM interactions, researchers \should report the full interaction logs with an LLM if possible, that is not only the prompts but also the LLM's responses.
Reporting this is especially important when reporting a study targeting commercial SaaS solutions based on LLMs (e.g., ChatGPT) or novel tools that integrate LLMs via cloud APIs where there is even less guarantee of reproducing the state of the LLM-powered system at a later point by a reader of the study who wants to verify, reproduce, or replicate it. 
The rationale for this is similar to the rationale for reporting interview transcripts in qualitative research.
In both cases, it is important to document the entire interaction between the interviewer and the participant.
Just as a human participant might give different answers to the same question asked two months apart, the responses from ChatGPT can also vary over time.
Therefore, keeping a record of the actual conversation is crucial for accuracy and context and shows depth of engagement for transparency.


\subsubsection{Example(s)}

A recent paper by Anandayuvaraj et al.~\cite{anandayuvaraj2024fail} is a good example of making prompts available online. In the paper, the authors analyze software failures reported in news articles and use prompting to automate tasks such as filtering relevant articles, merging reports, and extracting detailed failure information. Their online appendix contains all the prompts used in the study, providing valuable transparency and supporting reproducibility.

A good example of comprehensive prompt reporting is provided by Liang et al.~\cite{Liang2024}. The authors make the exact prompts available in their online appendix on Figshare, including details such as code blocks being enclosed in triple backticks. While this level of detail would not fit within the paper itself, the paper thoroughly explains the rationale behind the prompt design and data output format. It also includes one overview figure and two concrete examples, ensuring transparency and reproducibility while keeping the main text concise.

An example for reporting full interaction logs is the study Ronanki et al.~\cite{ronanki2023investigating}, for which they reported the full answers of ChatGPT and uploaded them to \href{https://zenodo.org/records/8124936}{Zenodo}. 


\subsubsection{Advantages}

As motivated above, providing detailed documentation of prompts enhances verifiability, reproducibility, and comparability of LLM-based studies.
It allows other researchers to replicate the study under similar conditions, refine prompts based on documented improvements, and evaluate how different types of content (e.g., source code vs. execution traces) influence LLM behavior.
This transparency also enables a better understanding of how formatting, prompt length, and structure impact results across various studies.

An advantage of reporting full interaction logs is that, while for human participants conversations often cannot be reported due to confidentiality, LLM conversations can (e.g. as of beginning of 2025, OpenAI \href{https://openai.com/policies/sharing-publication-policy/}{allows sharing of chat transcripts}).
Detailed logs enable future reproduction or replication studies to compare results using the same prompts.
This could be valuable for tracking changes in LLM responses over time or across different versions of the model.
It would also enable secondary research to study how consistent an LLM's responses are over model versions and identify any variations or improvements in its performance for specific software engineering tasks.


\subsubsection{Challenges}

One challenge is the complexity of prompts that combine multiple components, such as code, error messages, and explanatory text.
Formatting differences—such as whether markdown or plain text was used—can affect how LLMs interpret inputs.
Additionally, prompt length constraints may require careful management, particularly for tasks involving extensive artifacts such as large codebases.
Privacy and confidentiality concerns can also hinder prompt sharing, especially when sensitive data is involved. In these cases, researchers \should provide anonymized examples and summaries wherever possible.
For prompts containing sensitive information, researchers \must: (i) anonymize personal identifiers. (ii) replace proprietary code with functionally equivalent examples or placeholders, and (iii) clearly mark modified sections.


Not all systems allow the reporting of full (system) prompts and interaction logs with the same ease.
This hinders transparency and verifiability.
Besides our recommendation to \openllm, researchers \must report whatever they can about commercial tools, acknowledging limitations due to unavailable data.
They might also consider using open-source tools such as Continue\footnote{https://blog.continue.dev/its-time-to-collect-data-on-how-you-build-software/}, which enable researchers to collect interaction logs and system prompts locally.
Understanding suggestions of commercial tools such as GitHub Copilot might require recreating the exact state of the codebase at the time the suggestion was made, a challenging context to report.
One solution could be to use version control to capture the exact state of the codebase when a recommendation occurred, and keep track of the files that were automatically added as context.


\subsubsection{Study Types}

Reporting requirements may vary depending on the study type.
For the study type \newtools, researchers \must explain how prompts were generated and structured within the tool.
When \llmusage, especially for controlled experiments exact prompts \must be reported for all conditions.
For observational studies, especially the ones targeting commercial tools, researchers \must report the full interactions logs/conversations except for when transcripts might identify anonymous participants or reveal personal or confidential information.
If the full interaction logs cannot be shared, e.g., because they contain confidential information, the prompts and responses \must at least be summarized and described in the \paper.

Researchers \must report the complete prompts that were used, including all instructions, context, and input data. This applies across all study types, but specific focus areas vary:

\begin{itemize}
  \item For LLMs as \annotators, researchers \must document any predefined coding guides or instructions included in prompts, as these influence how the model labels artifacts.
  \item For LLMs as \judges, researchers \must include the evaluation criteria, scales, and examples embedded in the prompt to ensure consistent interpretation.
  \item For LLMs used in \synthesis tasks (e.g., summarization, aggregation), researchers \must document the sequence of prompts used to generate and refine outputs, including follow-ups and clarifying queries.
  \item For LLMs as \participants (e.g., simulating human participants), researchers \must report any role-playing instructions, constraints, or personas used to guide LLM behavior.
  \item For \benchmarkingtasks studies using pre-defined prompts (e.g., HumanEval, CruxEval), researchers \must specify the benchmark version and any modifications made to the prompts or evaluation setup. If prompt tuning, retrieval-augmented generation (RAG), or other methods were used to adapt prompts, researchers \must disclose and justify those changes and \should make the relevant code publicly available.
\end{itemize}


\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}
