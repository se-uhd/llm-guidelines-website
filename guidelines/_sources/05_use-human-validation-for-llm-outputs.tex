\input{../../header.tex}

\begin{document}

\subsection{Use Human Validation for LLM Outputs}
\label{sec:use-human-validation-for-llm-outputs}

\begin{quote}
\underline{tl;dr:} \input{_tldr/05_use-human-validation-for-llm-outputs-tldr.tex}
\end{quote}

\subsubsection{Recommendations}

Although, in principle, LLMs can automate many tasks in research and software development, which have traditionally been performed by humans, it is often essential to assess the outcome quality to determine whether the automation was successful.
If, for a certain task, there are no reference datasets or suitable comparison metrics, researchers \should rely on human judgment to validate the LLM outputs.
Human judgment is particularly important if metrics and existing datasets alone cannot fully capture the target qualities relevant in the study context.
Integrating human participants in the study design will require additional considerations, including a recruitment strategy, annotation guidelines, training sessions, or ethical approvals.
Therefore, researchers \should consider human validation early in the study design, not as an afterthought.
In any way, they \must clearly define the constructs that the human and LLM annotators evaluate~\cite{DBLP:conf/ease/RalphT18}.
When designing custom instruments to assess LLM output (e.g., questionnaires, scales), researchers \should share their instruments in the \supplementarymaterial.

Validating the output of an LLM \may involve the aggregation of inputs from multiple human judges.
In these cases, researchers \should clearly describe their aggregation method and document their reasoning.
When multiple humans are annotating the same artifact, researchers \should validate the agreement between multiple validators with inter-rater reliability measures such as \emph{Cohen's Kappa} or \emph{Krippendorff's Alpha}.
When human validation is used, additional confounding factors \should be controlled for, e.g., by categorizing participants according to their level of experience or expertise.
Where applicable, researchers \should perform a power analysis to estimate the required sample size, ensuring sufficient statistical power in their experimental design.

Researchers \should use established reference models to compare humans with LLMs.
For example, the reference model of \citeauthor{Schneider2025ReferenceModel}~\cite{Schneider2025ReferenceModel} provides researchers with an overview of the design considerations for studies comparing LLMs with humans.
%They outline the relevant groups (researchers, experimenters, subjects), activities (study planning, task assignment, results analysis, interpretation), and artifacts (study design, tasks, results, conclusion).
%Further, they illustrate the relationship between these entities, the order of steps, and which design decisions need to be considered.
If studies involve the annotation of software artifacts, and the goal is to automate the annotation process using LLM, researchers \should follow systematic approaches to decide whether and how human annotators can be replaced.
For example, \citeauthor{DBLP:journals/corr/abs-2408-05534}~\cite{DBLP:journals/corr/abs-2408-05534} suggest a method that involves using a jury of three LLMs with 3 to 4 few-shot examples rated by humans, where the model-to-model agreement on all samples is determined using \emph{Krippendorff's Alpha}.
If the agreement is high (alpha $gt$ 0.5), a human rating can be replaced with an LLM-generated one.
In cases of low model-to-model agreement (alpha $\le$ 0.5), they then evaluate the prediction confidence of the model, selectively replacing annotations where the model confidence is high ($\ge$ 0.8).

\subsubsection{Example(s)}

\citeauthor{DBLP:journals/corr/abs-2408-05534}~\cite{DBLP:journals/corr/abs-2408-05534} evaluated how human annotations can be replaced by LLM generated labels.
In their study, they explicitly report the calculated agreement metrics between the models and between humans.
For example, the wrote that ``model-model agreement is high, for all criteria, especially for the three large models (GPT-4, Gemini, and Claude). Table I indicates that the mean Krippendorff's $\alpha$ is 0.68-0.76.  Second, we see that human-model and human-human agreements are in similar ranges, 0.24-0.40 and 0.21-0.48 for the first three categories.''

\citeauthor{DBLP:conf/icse/XueCBTH24}~\cite{DBLP:conf/icse/XueCBTH24} conducted a controlled experiment in which they evaluated the impact of ChatGPT on the performance and perceptions of students in an introductory programming course.
They used multiple measures to judge the impact of LLM from the human point of view.
In their study, they recorded students' screens, evaluated their answers for the given tasks, and distributed a post-study survey to collect their opinions.

%\citeauthor{hymel2025analysisllmsvshuman}~\cite{hymel2025analysisllmsvshuman} evaluated the capability of ChatGPT-4.0 to generate requirements documents. 
%Specifically, they evaluated two requirements documents based on the same business use case, one document generated with the LLM and one document created by a human expert.
%The documents were then reviewed by experts and judged in terms of alignment with the original business use case (scale of 1-10), requirements completion (Not Complete, Fairly Complete, Fully Complete), and whether they believed it was created by a human or an LLM.
%Finally, they analyzed the influence of the participants' familiarity with AI tools on the study results.
%Participants self-reported their AI tool proficiency (Novice, Intermediate, Advanced, Expert) and usage frequency (Daily, Weekly, Monthly, Never), which were then correlated with their ratings of the requirements documents.

\subsubsection{Advantages}

For empirical studies involving LLMs, human validation helps ensure the reliability of study results, as LLMs can produce incorrect or biased outputs.
For example, in natural language processing tasks, a large-scale study has shown that LLMs have a significant variation in their results, which limits their reliability as a direct substitute for human judges~\cite{DBLP:journals/corr/abs-2406-18403}. 
Moreover, LLMs do not match human annotation in labeling tasks for natural language inference, position detection, semantic change, and hate speech detection~\cite{DBLP:conf/chi/Wang0RMM24}.

Incorporating human judgment into the evaluation process adds a layer of quality control and increases the trustworthiness of the study findings, especially when explicitly reporting inter-rater reliability metrics~\cite{khraisha2024canlargelanguagemodelshumans}.
Incorporating feedback from individuals in the target population strengthens external validity by grounding study findings in real-world usage scenarios, which may positively impact the transfer of study results to practice.
Researchers might uncover additional opportunities to further improve the LLM or LLM-based tool based on the reported experiences.

\subsubsection{Challenges}

Measurement through human validation can be challenging.
Ensuring that the operationalization of a desired construct and the method of measuring it are appropriate requires a deep understanding of (1) the construct, (2) construct validity in general, and (3) systematic approaches for designing measurement instruments~\cite{DBLP:journals/tse/SjobergB23}.
Human judgment is subjective, which can lead to variability between judgments due to differences in experience, expertise, interpretations, and biases among evaluators~\cite{DBLP:journals/pacmhci/McDonaldSF19}.
For example, \citeauthor{hicks_lee_foster-marks_2025} found that \enquote{the struggle with adapting to AI-assisted work is more common for racially minoritized developers.
That group also rated the quality of the output of AI-assisted coding tools significantly lower than other groups}~\cite{hicks_lee_foster-marks_2025}.
Such biases can be mitigated with a careful selection and combination of multiple judges, but cannot be completely controlled for.
Recruiting participants as human validators will always incur additional resources compared to machine-generated measures.
Researchers must weigh the cost and time investment incurred by the recruitment process against the potential benefits for the validity of their study results.

\subsubsection{Study Types}

For \llmusage, researchers \should carefully reflect on the validity of their evaluation criteria.
In studies where there are arguably objective evaluation criteria (e.g., in \benchmarkingtasks), there is little need for human validation, since the benchmark has (hopefully) been validated.
Where criteria are more unclear (e.g., \annotators, \subjects, or \synthesis), human validation is important, and this guideline should be followed.
When using \judges, it is common to start with humans who co-create intial rating criteria. 
In developing \newtools, human validation of the LLM output is critical when the output is supposed to match human expectations.

\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}
