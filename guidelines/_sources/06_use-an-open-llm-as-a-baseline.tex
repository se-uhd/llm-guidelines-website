\input{../../header.tex}

\begin{document}

\subsection{Use an Open LLM as a Baseline}

\subsubsection{Recommendations}

\begin{quote}
\underline{tl;dr:} Researchers \should include an open LLM as a baseline when using commercial models in empirical studies and report inter-model agreement. By open LLM, we mean a model that everyone with the required hardware can deploy and operate. Such models are usually ``open weight.'' A full replication package with step-by-step instructions \should be provided as part of the \supplementarymaterial.
\end{quote}

Empirical studies using LLMs in software engineering, especially the ones targeting commercial tools or model, \should incorporate an open LLM as a baseline and report established metrics for inter-model agreement (see Section \benchmarksmetrics).
We acknowledge that including an open LLM baseline might not always be possible, for example if the study involves human participants and letting them work on the tasks using two different models might not be feasible. Another case where this may not be necessary is if the use of the LLM is tangential to the study, or strict reproducibility of the LLM portion of the results is not necessary.

Open models allow other researchers to verify research results and build upon them, even without access to commercial models.
Comparing commercial and open models also allows to contextualize model performance.
Researchers \should provide a full replication package as part of their \supplementarymaterial, including clear, step-by-step instructions on how to verify and reproduce the results reported in the paper.

Open LLMs are available on platforms such as \href{https://huggingface.co/}{Hugging Face}.
Depending on the size, and the local computing power, open LLMs can be hosted on a local computer or server using frameworks such as \href{https://ollama.com/}{Ollama} or \href{https://lmstudio.ai/}{LM Studio}.
They can also be run on cloud-based services such as \href{https://together.ai/}{Together AI} or large hyperscalers such as AWS, Azure, Alibaba Cloud, and Google Cloud.

The term ``open'' can have different meanings in context of LLMs.
Widder et al. discuss three types of openness: transparency, reusability and extensibility~\cite{widder2024open}.
They also discuss what openness in AI can and cannot provide.
Moreover, the Open Source Initiative (OSI) \cite{OSIAI2024} provides a definition of open-source AI that serves as a useful framework for evaluating the openness of AI models.
In simple terms, according to OSI, open-source AI means  you have access to everything you need to use the AI, such as understanding, modifying, sharing, retraining, and recreating.


\subsubsection{Example(s)}

Numerous recent studies have adopted open‐source LLMs as baseline models. For instance, Wang et al. \cite{wang2024and} evaluated seven advanced LLMs---six of which were open source---by testing 145 API mappings drawn from eight popular Python libraries across 28,125 completion prompts aimed at detecting deprecated API usage in code completion. Moumoula et al. \cite{moumoula2024large} compared four LLMs on a cross-language code clone detection task, finding that three evaluated models were open source. Similarly, Gonçalves et al. \cite{gonccalves2025evaluating} fine-tuned LLaMA 3.2 on a refined version of the DiverseVul dataset to benchmark vulnerability detection performance. Many papers \cite{DBLP:journals/jss/YangZCZHC23, DBLP:conf/gaiis/XiaSD24, DBLP:conf/kbse/SonnekalbGBM22, DBLP:conf/icse/CaiYMMN24} used CodeBERT as an LLM, which is a bimodal (code + NL) Transformer pre-trained by Microsoft Research and released under the MIT license. Its model weights, source code, and data-processing scripts are all openly published on GitHub\footnote{\url{https://github.com/microsoft/CodeBERT}} (\modelversion).


\subsubsection{Advantages}

Regarding the advantages of using an open LLM as a baseline, this enhances the reproducibility of scientific research by providing full access to model architectures, training data, and parameter settings (see Section \modelversion), thereby enabling independent reconstruction and verification of experimental results. Moreover, by adopting a standard, open-source baseline, researchers can conduct more objective evaluations and directly compare novel methods against established performance metrics without the variability introduced by proprietary systems (see Section \benchmarksmetrics). The transparent nature of these models allows detailed inspection of the data processing pipelines and decision-making routines, essential for identifying potential sources of bias and delineating model limitations (see Section \limitationsmitigations). Furthermore, unlike closed-source alternatives that may be withdrawn or altered without notice, open LLMs ensure long-term accessibility and stability, preserving critical resources for future studies. Finally, the relaxed licensing requirements and minimal fees associated with open-source implementations lower financial barriers, making advanced language models attainable for research groups operating under constrained budgets.


\subsubsection{Challenges}

Open-source large language models face several notable challenges (see Section \limitationsmitigations). First, they often lag behind the most advanced proprietary systems regarding accuracy and computational efficiency, making it difficult for researchers to demonstrate clear improvements when benchmarking new methods (see Section \benchmarksmetrics). Additionally, deploying and experimenting with these models typically requires substantial hardware resources---high-performance GPUs, ample memory capacity, and optimised infrastructure, which may be beyond the reach of many academic groups (see Section \toolarchitecture). 

The notion of “openness” itself remains in flux: although numerous models are described as open, many release only the trained weights without disclosing the underlying training data or methodological details (see Section \modelversion), a practice sometimes referred to as ``open weight'' openness \cite{Gibney2024}. To address this gap, we reference the Open Source AI Definition proposed by the Open Source Initiative as an initial framework for what constitutes genuinely open-source AI software \cite{OSIAI2024}. 

Finally, unlike managed cloud APIs provided by proprietary vendors, installing, configuring, and fine-tuning open-source models can be technically demanding---documentation is often sparse or fragmented, placing a high barrier to entry for researchers without specialised engineering support (see Section \limitationsmitigations).

\subsubsection{Study Types}

When evaluating \newtools, researchers \should \openllm whenever it is technically feasible; if integration proves too complex, they \must report the initial benchmarking results of open models (see Section \benchmarksmetrics). In formal benchmarking studies and controlled experiments (see Section \benchmarkingtasks), an open LLM \must be one of the models under evaluation. For observational studies in which an open LLM is impossible, investigators \should explicitly acknowledge its absence and discuss how this limitation might affect their conclusions (see Section \limitationsmitigations). Finally, in qualitative research, where an LLM serves to explore data or contrast alternative interpretations (see Section \synthesis), reporting an Open LLM is optional but can provide valuable context.


\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}
