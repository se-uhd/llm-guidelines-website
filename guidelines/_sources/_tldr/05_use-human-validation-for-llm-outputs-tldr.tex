If assessing the quality of generated artifacts is important and no reference datasets or suitable comparison metrics exist, researchers \should use human validation for LLM outputs. If they do, they \must define the measured construct (e.g., usability, maintainability) and describe the measurement instrument in the \paper. Researchers \should consider human validation early in the study design (not as an afterthought), build on established reference models for human-LLM comparison, and share their instruments as \supplementarymaterial. When aggregating LLM judgments, methods and rationale \should be reported and inter-rater agreement \should be assessed. Confounding factors \should be controlled for, and power analysis \should be conducted to ensure statistical robustness.