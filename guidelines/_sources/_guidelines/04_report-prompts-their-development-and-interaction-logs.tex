\guidelinesubsubsection{Rationale}

Prompts are critical for any study involving LLMs~\cite{DBLP:conf/iclr/Sclar0TS24}.
Depending on the task, they may include instructions, context (e.g., source code, execution traces, error messages), input data, and output indicators, with outputs ranging from unstructured text to structured formats such as JSON.\footnote{\url{https://www.promptingguide.ai/introduction/elements}}
Prompts significantly influence the quality of the output of a model, and understanding how exactly they were formatted and integrated into an LLM-based study is essential to ensure transparency, verifiability, and reproducibility.
\citeauthor{DBLP:conf/iclr/Sclar0TS24} question the methodological validity of comparing models with \enq{an arbitrarily chosen, fixed prompt format}, because their research has shown that the performance of different prompt formats only weakly correlates between different models~\cite{DBLP:conf/iclr/Sclar0TS24}.
We remind the reader that these guidelines do not apply when LLMs are used solely for language polishing, paraphrasing, translation, tone or style adaptation, or layout edits (see \scope).
This implies that our guidelines do not recommend that researcher disclose the prompts they have used for such tasks.

\guidelinesubsubsection{Recommendations}

\paragraph{Prompt Reporting:}
Researchers \must report all prompts that were used for an empirical study, including all instructions, context, input data, and output indicators.
The only exception to this is if transcripts might identify anonymous participants or reveal personal or confidential information, for example, when working with industry partners.
Prompts can be reported using a structured template that contains placeholders for dynamically added content.
Moreover, researchers \should specify the exact formatting of prompts, including how code snippets were enclosed (e.g., markdown-style code blocks, triple backticks), whether whitespace was preserved, and how other artifacts such as error messages and stack traces were presented.

\paragraph{Prompt Development:}
Researchers \must also explain in the \paper how they developed the prompts and why they decided to follow certain prompting strategies.
If prompts from the early phases of a research project are unavailable, they \must at least summarize the prompt evolution.
However, given that prompts can be stored as plain text, there are established SE techniques such as version control systems that can be used to collect the required provenance information.

Prompt development is often iterative, involving collaboration between human researchers and AI tools.
Researchers \should report any instances in which LLMs were used to suggest prompt refinements and how these suggestions were incorporated.
Prompts may need revision in response to failure cases, and pilot testing is vital to ensure reliable results.
If such testing was conducted, researchers \should summarize key insights, including how different prompt variations affected output quality and which criteria were used to finalize the prompt design.
%Iterative changes based on human feedback and pilot testing results \should be included in the documentation.
A prompt changelog can track prompt evolution, including key revisions and reasons for changes (e.g., v1.0: initial prompt; v2.0: incorporated examples of ideal responses).
Since prompt effectiveness varies between models and model versions, researchers \must make clear which prompts were used for which models in which versions and with which configuration.

\paragraph{Prompting Strategy and Input Handling:}
Researchers \must specify whether zero-shot, one-shot, or few-shot prompting was used.
For few-shot prompts, the examples provided to the model \should be clearly outlined, along with the rationale for selecting them.
If multiple versions of a prompt were tested, researchers \should describe how these variations were evaluated and how the final design was chosen.

When dealing with extensive or complex prompt context, researchers \must describe the strategies they used to handle input length constraints.
Approaches might include truncating, summarizing, or splitting prompts into multiple parts.
Token optimization measures, such as simplifying code formatting or removing unnecessary comments, \must also be documented if applied.

\paragraph{Dynamic and User-Authored Prompts:}
In cases where prompts are generated dynamically, such as through preprocessing, template structures, or retrieval-augmented generation (RAG), the process \must be thoroughly documented.
This includes explaining any automated algorithms or rules that influenced prompt generation.
For studies involving human participants who create or modify prompts, researchers \must describe how these prompts were collected and analyzed.

\paragraph{Supplementary Material and Anonymization:}
To ensure complete reproducibility, researchers \must make all prompts and prompt variations publicly available as part of their \supplementarymaterial.
If the complete set of prompts cannot be included in the \paper, researchers \should provide summaries and representative examples.
This also applies if complete disclosure is not possible due to privacy or confidentiality concerns.
For prompts containing sensitive information, researchers \must: (1) anonymize personal identifiers, (ii) replace proprietary code with placeholders, and (iii) clearly highlight modified sections.

\paragraph{Interaction Logs:}
When trying to verify results, even with the exact same prompts, decoding strategies, and parameters, LLMs can still behave non-deterministically.
Non-determinism can arise from batching, input preprocessing, and floating point arithmetic on GPUs~\cite{Chann2023}.
Thus, in order to enable other researchers to verify the conclusions that researchers have drawn from LLM interactions, researchers \should report the full interaction logs (prompts and responses) as part of their \supplementarymaterial.
Reporting this is especially important for studies targeting commercial software-as-a-service (SaaS) solutions such as ChatGPT.
%or novel tools that integrate LLMs via cloud APIs where there is even less guarantee of reproducing the state of the LLM-powered system at a later point by a reader of the study who wants to verify, reproduce, or replicate it. 
The rationale for this is similar to the rationale for reporting interview transcripts in qualitative research.
Just as a human participant might give different answers to the same question asked two months apart, the responses from tools such as ChatGPT can also vary over time.
%Therefore, keeping a record of the actual conversation is crucial for accuracy and context and shows depth of engagement for transparency.

\paragraph{Agentic Systems:}
For agentic systems that autonomously plan and execute tasks, the developed plans \should be reported if available, as they document the sequence of actions the system chose to pursue and the intermediate goals it set.
Interaction logs \should be generalized to include full interaction traces between humans and agentic systems, capturing human-in-the-loop feedback, approval or rejection decisions, and iterative refinements.
These traces complement agent behavior traceability requirements and support the evaluation of agentic tool outputs.

\paragraph{Context Files:}
AI coding agents such as \emph{Claude Code} can be configured using version-controlled Markdown files (e.g., \texttt{CLAUDE.md} or \texttt{AGENTS.md}) that provide persistent, project-specific instructions to the agent~\cite{mohsenimofidi2026context}.
These \emph{context files} are automatically injected into agent prompts and can significantly influence agent behavior by specifying coding conventions, architectural constraints, tool preferences, and task-specific instructions.
Researchers \must report all context files used to configure AI agent behavior as part of their \supplementarymaterial, as these files are a form of prompt artifact that shapes agent behavior similarly to system prompts.


\guidelinesubsubsection{Example(s)}

A paper by \citeauthor{anandayuvaraj2024fail}~\cite{anandayuvaraj2024fail} is a good example of making prompts available online.
In that paper, the authors analyze software failures reported in news articles and use prompting to automate tasks such as filtering relevant articles, merging reports, and extracting detailed failure information.
Their online appendix contains all the prompts used in the study, providing transparency and supporting reproducibility.

\citeauthor{Liang2024}'s paper is another good example of comprehensive prompt reporting.
The authors make the exact prompts available in their \supplementarymaterial on Figshare, including details such as code blocks being enclosed in triple backticks.
The \paper thoroughly explains the rationale behind the prompt design and the data output format.
It also includes an overview figure and two concrete examples, enabling transparency and reproducibility while keeping the main text concise.

An example of reporting full interaction logs is the study by \citeauthor{ronanki2023investigating}~\cite{ronanki2023investigating}, for which they reported the full answers of ChatGPT and uploaded them to \href{https://zenodo.org/records/8124936}{Zenodo}. 

\guidelinesubsubsection{Benefits}

Detailed prompt documentation improves verifiability, reproducibility, and comparability of LLM-based studies, allowing other researchers to replicate studies, refine prompts, and evaluate how different content types and formatting choices influence LLM behavior.

Unlike human participant conversations, which often cannot be reported due to confidentiality, LLM interaction logs can be shared. This enables reproduction studies, tracking of response changes over time or across model versions, and secondary research on LLM consistency for specific SE tasks.

\guidelinesubsubsection{Challenges}

One challenge is the complexity of prompts that combine multiple components, such as code, error messages, and explanatory text.
Formatting differences (e.g., Markdown vs. plain text) can affect how LLMs interpret input.
Additionally, prompt length constraints may require careful context management, particularly for tasks involving extensive artifacts such as large codebases.
Privacy and confidentiality concerns can hinder prompt sharing, especially when sensitive data is involved.

Not all systems allow the reporting of complete (system) prompts and interaction logs with ease.
This hinders transparency and verifiability.
We also recommend exploring open-source tools such as \emph{OpenCode}~\cite{opencode}.
For commercial tools, researchers \must report all available information and acknowledge unknown aspects as limitations.
Understanding suggestions of commercial tools such as \emph{GitHub Copilot} might require recreating the exact state of the codebase at the time the suggestion was made, a challenging context to report.
One solution could be to use version control to capture the exact state of the codebase when a recommendation was made, keeping track of the files that were automatically added as context.

\guidelinesubsubsection{Study Types}

Researchers \must report the complete prompts that were used, including all instructions, context, and input data. This applies across all study types, but reporting requirements and specific focus areas vary.

For \newtools, researchers \must explain how prompts were generated and structured within the tool.
For \llmusage, especially for controlled experiments, exact prompts \must be reported for all conditions.
For observational studies, especially the ones targeting commercial tools, researchers \must report the full interaction logs except for when transcripts might identify anonymous participants or reveal personal or confidential information.
As mentioned before, if the complete interaction logs cannot be shared, e.g., because they contain confidential information, the prompts and responses \must at least be summarized and described in the \paper.
For \annotators, researchers \must document any predefined coding guides or instructions included in prompts, as these influence how the model labels artifacts.
For \judges, researchers \must report the evaluation criteria, scales, and examples embedded in the prompt to ensure a consistent interpretation.
For \synthesis tasks (e.g., summarization, aggregation), researchers \must document the sequence of prompts used to generate and refine outputs, including follow-ups and clarification queries.
For \subjects (e.g., simulating human participants), researchers \must report any role-playing instructions, constraints, or personas used to guide LLM behavior.
For \benchmarkingtasks studies using pre-defined prompts (e.g., \emph{HumanEval}, \emph{SWE-Bench}), researchers \must specify the benchmark version and any modifications made to the prompts or evaluation setup.
If prompt tuning, retrieval-augmented generation (RAG), or other methods were used to adapt prompts, researchers \must disclose and justify those changes, and \should make the relevant code publicly available.

%% Suggestion: Consider integrating the footnote defining "over-rationalized" into the main text for smoother reading.
%% Suggestion: Consider splitting into two paragraphs: (1) what NOT to demand from authors; (2) what TO focus on (reproducibility + bias transparency).
\guidelinesubsubsection{Advice for Reviewers}

As with other guidelines, missing prompt information is typically a minor revision request unless so much is missing that methodological rigor cannot be assessed. The most challenging aspect is the description of prompt development, which is inherently iterative and creative. Reviewers should \textit{not} expect justification of each word choice or post-hoc rationalized accounts of prompt generation. Instead, reviewers should focus on whether (1) a new research team could \textit{use} (not reproduce) exactly the same prompts in exactly the same way; and (2) potential biases or validity issues are transparent.

\guidelinesubsubsection{See Also}
\begin{itemize}[label=$\rightarrow$]
    \item Section~\modelversion: model versions and configuration for prompt-model mapping.
    \item Section~\toolarchitecture: agent behavior traceability requirements.
    \item Section~\humanvalidation: evaluation of agentic tool outputs.
    \item Section~\openllm: open-source tools for transparent prompt logging.
    \item Section~\limitationsmitigations: reporting limitations of commercial tool transparency.
\end{itemize}
