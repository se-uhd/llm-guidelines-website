\guidelinesubsubsection{Rationale}

LLMs and LLM-based tools are frequently updated, and configuration parameters such as temperature or seed values affect content generation. 
This guideline focuses on documenting the \emph{model-specific} aspects of empirical studies involving LLMs, concentrating on the models themselves, their version, configuration parameters, and customizations (e.g., fine-tuning). While the \toolarchitecture section addresses system-level integration, the information outlined here is always essential for reproducibility whenever an LLM is involved.

\guidelinesubsubsection{Recommendations}

Researchers \must document in the \paper which model or tool version they used in their study, along with the date when the experiments were carried out and the configured parameters that affect output generation.
Since default values might change over time, researchers \should always report all configuration values, even if they used the defaults.
Checksums and fingerprints \should be reported since they identify specific versions and configurations.
Depending on the study context, other properties such as the context window size (number of tokens) \should be reported.
When using quantized models, researchers \should report the quantization level (e.g., 4-bit, 8-bit) and method (e.g., GPTQ or AWQ), as different quantization approaches produce different outputs, affecting both output quality and reproducibility.
Researchers \should motivate in the \paper why they selected certain models, versions, and configurations.
Reasons may be monetary, technical, or methodological (e.g., planned comparison to previous work).
Depending on the specific study context, additional information regarding the experiment or tool architecture \should be reported.

A common customization approach for existing LLMs is fine-tuning.
If a model was fine-tuned, researchers \must describe the fine-tuning goal (e.g., improving the performance for a specific task), the fine-tuning procedure (e.g., full fine-tuning vs. Low-Rank Adaptation (LoRA), selected hyperparameters, loss function, learning rate, batch size, etc.), and the fine-tuning dataset (e.g., data sources, the preprocessing pipeline, dataset size) in the \paper.
Researchers \should either share the fine-tuning dataset as part of the \supplementarymaterial or explain in the \paper why the data cannot be shared (e.g., because it contains confidential or personal data that could not be anonymized).
The same applies to the fine-tuned model weights.
Suitable benchmarks and metrics \should be used to compare the base model with the fine-tuned model.

In summary, our recommendation is to report:
\begin{enumerate*}[label=(\arabic*)]
\item Model/tool name and version (\must in \paper);
\item All relevant configured parameters that affect output generation (\must in \paper);
\item Default values of all available parameters (\should);
\item Checksum/fingerprint of used model version and configuration (\should);
\item Additional properties such as context window size (\should);
\item Model quantization level and method, if applicable (\should).
\end{enumerate*}

For fine-tuned models, additional recommendations apply:
\begin{enumerate*}[label=(\arabic*)]
\item Fine-tuning goal (\must in \paper);
\item Fine-tuning dataset creation and characterization (\must in \paper);
\item Fine-tuning parameters and procedure (\must in \paper);
\item Fine-tuning dataset and fine-tuned model weights (\should);
\item Validation metrics and benchmarks (\should).
\end{enumerate*}

Commercial models (e.g., GPT-5) or LLM-based tools (e.g., ChatGPT) might not give researchers access to all required information.
Our suggestion is to report what is available and openly acknowledge limitations that hinder reproducibility.

\guidelinesubsubsection{Example(s)}

Based on the documentation that OpenAI and Azure provide~\cite{OpenAI25, Azure25}, researchers might, for example, report:

\begin{quote}
\it
 ``We integrated a  \texttt{gpt-4} model in version \texttt{0125-Preview} via the Azure OpenAI Service, and configured it with a temperature of 0.7, top\_p set to 0.8, a maximum token length of 512, and the  seed value \texttt{23487}.
 We ran our experiment on 10th January 2025. The system fingerprint was \texttt{fp\_6b68a8204b}.
\end{quote}

\citeauthor{DBLP:conf/icse/KangYY23} provide a similar statement in their paper on exploring LLM-based bug reproduction~\cite{DBLP:conf/icse/KangYY23}:

\begin{quote}
\it
``We access OpenAI Codex via its closed beta API, using the code-davinci-002 model. For Codex, we set the temperature to 0.7, and the maximum number of tokens to 256.''
\end{quote}

Our guidelines additionally suggest to report a checksum/fingerprint and exact dates, but otherwise this example is close to our recommendations. 

\citeauthor{DBLP:conf/icsa/DharVV24}~\cite{DBLP:conf/icsa/DharVV24} assessed whether LLMs can generate architectural design decisions, detailing the system architecture and the LLM's role within it.
They provide information on the fine-tuning approach and datasets, including the source of architectural decision records, preprocessing methods, and data selection criteria.

For self-hosted models, the \supplementarymaterial can become a true replication package. For example, for models provisioned using \href{https://ollama.com/library/}{ollama}, one can report the specific tag and checksum, e.g., \emph{``llama3.3, tag 70b-instruct-q8\_0, checksum d5b5e1b84868.''}
Given suitable hardware, running the model is then as easy as executing the following command:\\
\texttt{ollama run llama3.3:70b-instruct-q8\_0}

\guidelinesubsubsection{Benefits}

Reporting this information is a prerequisite for the verification, reproduction, and replication of LLM-based studies. While LLMs are inherently non-deterministic, this cannot excuse dismissing reproducibility. Although exact reproducibility is hard to achieve, reporting the information outlined in this guideline helps researchers come as close as possible to that standard.
%However, that information alone is generally not sufficient.

\guidelinesubsubsection{Challenges}

Different model providers and modes of operating the models allow for varying degrees of information.
For example, OpenAI provides a model version and a system fingerprint describing the backend configuration, which can also influence the output.
However, in fact, the fingerprint is intended only to detect changes in the model or its configuration; one cannot go back to a certain fingerprint.
As a beta feature, OpenAI lets users set a seed parameter to receive ``(mostly) consistent output''~\cite{OpenAI23}.
However, the seed value does not allow for full reproducibility and the fingerprint changes frequently. 
Although, as motivated above, open models significantly simplify re-running experiments, they also come with challenges in terms of reproducibility, as generated outputs can be inconsistent despite setting the temperature to 0 and using a seed value (see \href{https://github.com/ollama/ollama/issues/5321}{GitHub issue for Llama3}).
Setting the temperature to 0 configures greedy decoding (always selecting the most probable next token), which minimizes output variability but can degrade quality by producing repetitive text and missing higher-quality responses~\cite{holtzman2020curious}.

Even with a temperature of 0, full determinism is rarely guaranteed: floating-point arithmetic on GPUs causes slight numerical differences that cascade into divergent token selections~\cite{YuanNondeterminismLLMInference}, Sparse Mixture-of-Experts routing amplifies this effect~\cite{Chann2023}, silent backend changes in commercial APIs produce different outputs over time~\cite{DBLP:journals/corr/abs-2307-09009}, and even self-hosted open models with identical settings do not always yield consistent outputs~\cite{DBLP:journals/corr/abs-2510-25506, DBLP:conf/naacl/SongWLL25}.
Researchers should therefore not treat a temperature of 0 as a guarantee of reproducibility, but as one measure among several, including fixed seed values~\cite{OpenAI23}, system fingerprints, and archiving of raw outputs.
When a temperature of 0 is chosen primarily for reproducibility, this motivation \should be stated explicitly, along with an acknowledgment of its potential impact on output quality.
%\comment{Should we also address how API rate limits and usage constraints can affect experiments?}

\guidelinesubsubsection{Study Types}

This guideline \must be followed for all study types for which the researcher has access to (parts of) the model's configuration.
They \must always report the configuration that is visible to them, acknowledging the reproducibility challenges of commercial tools and models that are offered as-a-service.
Depending on the specific study type, researchers \should provide additional information on the architecture of a tool they built (see \toolarchitecture), prompts and interactions logs (see \prompts), and specific limitations and mitigations (see \limitationsmitigations).

For example, when \llmusage by focusing on commercial tools such as ChatGPT or GitHub Copilot, researchers \must be as specific as possible in describing their study setup.
The configured model name, version, and the date when the experiment was conducted \must always be reported.
In those cases, reporting other aspects, such as prompts and interaction logs, is essential.

For \annotators, \judges, and \synthesis, researchers \must report the model configuration used for the respective annotation, judging, or synthesis tasks, including temperature and other sampling parameters that affect output variability.
For \subjects, researchers \must report any persona-related configuration settings and parameters that shape the simulated behavior.
For \newtools, researchers \must report the configuration for each model integrated in the tool's architecture, including any model-specific parameter choices.
For \benchmarkingtasks, researchers \must report the configuration for all benchmarked models to enable fair cross-model comparisons.

\guidelinesubsubsection{Advice for Reviewers}

Missing version, configuration, or parameter information is typically a minor revision request. Before concluding that information is absent, reviewers should check appendices and supplementary materials, as details are sometimes reported there rather than in the main text. Rejection over missing details is rarely warranted unless the omissions obscure deeper methodological problems.

\guidelinesubsubsection{See Also}
\begin{itemize}[label=$\rightarrow$]
    \item Section~\toolarchitecture: system-level architecture beyond the model.
    \item Section~\prompts: prompt reporting and development.
    \item Section~\benchmarksmetrics: benchmarks for comparing base and fine-tuned models.
    \item Section~\openllm: open models for self-hosted reproducibility.
    \item Section~\limitationsmitigations: reporting reproducibility limitations.
\end{itemize}