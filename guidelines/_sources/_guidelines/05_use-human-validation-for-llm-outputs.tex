\guidelinesubsubsection{Rationale}

Automated metrics alone cannot fully capture the validity of subjective or complex constructs.
When LLMs are used to automate tasks that involve human judgment, their outputs must be validated against human assessments to ensure that the results are not only reliable but also valid.

\guidelinesubsubsection{Recommendations}

When LLMs are used to automate research or software development tasks previously performed by humans, it is essential to assess the LLM's performance. When existing reference datasets or comparison metrics cannot fully capture the target qualities relevant in the study context, researchers \should rely on human judgment to validate the LLM outputs.

\paragraph{Study Design Considerations:}
Integrating human participants in the study design will require additional considerations, including a recruitment strategy, annotation guidelines, training sessions, or ethical approvals.
Therefore, researchers \should consider human validation early in the study design, not as an afterthought.
Authors \must clearly define the constructs that the human and LLM annotators evaluate~\cite{DBLP:conf/ease/RalphT18}.
When designing custom instruments to assess LLM output (e.g., questionnaires, scales), researchers \must share their instruments, for example in the \supplementarymaterial.

\paragraph{Subjective Judgment and Agreement:}
When the judgment is subjective (i.e., depends on the judge's values or theories), the LLM output \should be evaluated against judgments aggregated from multiple human judges, and researchers \should clearly describe their aggregation method and reasoning. The preferred method is to randomly order the objects requiring judgment and put them in groups small enough to judge in one to three hours. Two or three human experts review the first group together, simultaneously judging, discussing, and creating a set of decision rules documenting their reasoning. Then, the experts iterate among rounds of:
\begin{enumerate}
    \item independently rating one group of objects,
    \item calculating inter-rater agreement (IRA) or reliability (IRR),
    \item meeting to discuss disagreements and reach consensus,
    \item updating the decision rules so the disagreement will not recur.
\end{enumerate}

The goal is to reach a target IRA or IRR (e.g., Krippendorff's $\alpha>0.8$), indicating sufficient decision rules. Once this target is reached and sustained for two to three groups, it is permissible to continue with a single rater.
Researchers \should report measures of IRA or IRR, preferably broken down by round.

Confounding factors \should be discussed and, where feasible, controlled for (e.g., by categorizing participants according to their level of experience or expertise).
Where applicable, researchers may perform a power analysis~\cite{Cohen1992,DBLP:journals/infsof/DybaKS06} to estimate the required sample size, ensuring sufficient statistical power in their experimental design.
However, we also note that to our knowledge, there is limited established guidance specific to determining sample sizes for LLM-human comparisons. In other fields, 100 comparisons are often made without further explanation~\cite{tam2024framework}.

Researchers \should use established reference models to compare humans with LLMs.
For example, \citeauthor{Schneider2025ReferenceModel}~\cite{Schneider2025ReferenceModel} outline design considerations for studies comparing LLMs with humans.
If studies aim to automate annotating software artifacts using an LLM, researchers \should follow systematic approaches to decide whether and how human annotators can be replaced (e.g., showing that a jury of three LLMs exhibit model-to-model agreement comparable to trained human annotators, such as Krippendorff's $\alpha>0.8$). Model-to-model agreement should not be held to a lower standard than human-to-human agreement, as low thresholds may simply reflect shared model biases rather than annotation quality.

\paragraph{Agentic Tools}
Besides generating and modifying content, agentic software development tools such as \emph{Claude Code} can autonomously call command-line tools or pull in additional information from MCP servers.
For such tools, it makes sense to separate textual output (e.g., file changes) from output related to tool execution.
When evaluating agentic tools, researchers \should assess the feedback that users provided for proposed changes, report statistics on how frequently they accepted the content, and how they modified it.
This agentic human-in-the-loop interaction approach is essentially a built-in human validation, even though the degrees of freedom are larger than in more traditional experiments that use LLMs directly.

\guidelinesubsubsection{Example(s)}

\citeauthor{DBLP:conf/msr/AhmedDTP25}~\cite{DBLP:conf/msr/AhmedDTP25} proposed a systematic method for deciding whether LLMs can replace human annotators on a given task, using model-to-model agreement as an initial screening criterion (with a threshold of $\alpha>0.5$) and model confidence for sample-level decisions. However, their threshold is well below the levels generally considered acceptable for inter-rater agreement.
\citeauthor{Krippendorff2018} recommends discarding data with $\alpha<0.667$, considers $0.667 \leq \alpha < 0.8$ sufficient only for tentative conclusions, and requires $\alpha \geq 0.8$ for reliable data~\cite{Krippendorff2018}. Researchers adopting similar screening approaches should apply thresholds consistent with these established standards. Notably, while three LLMs exhibited higher inter-model agreement than human annotators on some tasks, human-model agreement remained low on others.
This illustrates that high model-model reliability does not guarantee alignment with human judgment, reinforcing the need for human validation before assuming that LLM annotations are valid.
Moreover, a high model-model agreement could merely indicate that the models share systematic biases and hence reliably agree on the wrong answer.

%Meanwhile, \citeauthor{DBLP:conf/icse/XueCBTH24}~\cite{DBLP:conf/icse/XueCBTH24} conducted a controlled experiment to evaluate the impact of ChatGPT on the performance and perceptions of students in an introductory programming course.
%They used multiple measures to judge LLM impacts from the human point of view including recording students' screens, evaluating their answers for given tasks, and distributing an opinion survey.
\citet{hymel2025analysisllmsvshuman}~\cite{hymel2025analysisllmsvshuman} evaluated ChatGPT's capability to generate requirements documents by comparing an LLM-generated and a human-generated document based on the same business use case. Domain experts reviewed both documents and attempted to distinguish their origin.% and judged in terms of alignment with the original business use case (scale of 1-10), requirements completion (Not Complete, Fairly Complete, Fully Complete), and whether they believed it was created by a human or an LLM.
%Finally, they analyzed the influence of the participants' familiarity with AI tools on the study results.
%Participants self-reported their AI tool proficiency (Novice, Intermediate, Advanced, Expert) and usage frequency (Daily, Weekly, Monthly, Never), which were then correlated with their ratings of the requirements documents.

\guidelinesubsubsection{Benefits}

Validating LLMs against human judgments builds confidence in the LLM's accuracy and validity, increasing the trustworthiness of findings, especially when IRA or IRR metrics are reported~\cite{khraisha2024canlargelanguagemodelshumans}. Furthermore, incorporating feedback from human judges may help to improve the LLM or LLM-based tool based on the reported experiences.

\guidelinesubsubsection{Challenges}

Assessing a construct like LLM performance is challenging because ensuring that a construct is defined well and operationalized using an appropriate measurement model requires a deep understanding of (1) the construct, (2) construct validity in general, and (3) instrumentation~\cite{DBLP:journals/tse/SjobergB23,ralph2024teaching}. Comparing an LLM to human judges is typically slower and more expensive than machine-generated measures. More fundamentally, neither human judgment nor machine-generated measures provides an objective ground truth against which LLM accuracy can be firmly determined.   

Human judgments exhibit variability due to differences in experience, expertise, interpretations, and personal biases~\cite{DBLP:journals/pacmhci/McDonaldSF19}. When diverse humans rate items reliably given clear decision rules, we assume that reliability implies validity, but it does not. Measuring reliability is \textit{much} easier than measuring validity, and often the best researchers can do is argue conceptually for why their judges, decision rules, and constructs \textit{should} produce valid ratings.

Researchers must weigh the cost and time need for human validation against the expected corresponding improvement in validity over machine-generated measures. 

%For example, LLM results vary significantly in natural language processing tasks, casting doubt on their reliability and the possibiltiy of replacing human judges~\cite{DBLP:journals/corr/abs-2406-18403}. Moreover, LLMs do not match human annotation in labeling tasks for natural language inference, position detection, semantic change, and hate speech detection~\cite{DBLP:conf/chi/Wang0RMM24}.

\guidelinesubsubsection{Study Types}

This guideline applies to all study types, although the need for human validation varies.
For \annotators, researchers \should validate LLM-generated annotations against human annotators to assess labeling quality and identify systematic biases.
When using \judges, researchers \should co-create initial rating criteria with humans and validate a sample of LLM judgments against human expert assessments.
For \synthesis, researchers \should employ human oversight to verify that qualitative interpretations and synthesized outputs faithfully represent the underlying data.
For \subjects, researchers \should validate simulated responses against real human data to assess the fidelity of the simulation.
For \llmusage, researchers \should carefully reflect on the validity of their evaluation criteria and validate subjective assessments with human experts.
In developing \newtools, human validation of the LLM output is critical when the output is supposed to match human expectations.
For \benchmarkingtasks, there is less need for human validation when using extensively validated and widely-used benchmarks, but researchers \should employ human validation when creating or adapting new benchmarks.

\guidelinesubsubsection{Advice for Reviewers}

Human validation may be the most challenging of our guidelines to assess because it often requires evaluating conceptual arguments. If LLM output is validated only by comparison with other LLMs, reviewers should look for \textit{quantitative empirical evidence} that such comparison is reliable \textit{and} valid. High inter-model agreement alone is insufficient, as reliability does not imply validity. Similarly, reviewers should expect evidence that any employed benchmarks are reliable and valid. Absent such evidence, human validation is warranted.
A single human judge is appropriate only when judgments depend on widely accepted theories and involve limited value conflict (e.g., tagging method names containing abbreviations). For multiple judges, reviewers should expect IRA/IRR improvement techniques as described in the recommendations above (experienced raters, organized rounds, consensus meetings, updated decision rules). Low IRA or IRR (e.g., Krippendorff's $\alpha<0.8$) without these techniques is a concern. Conversely, if authors have followed best practices and still obtained mediocre results (e.g., $0.66<\alpha<0.8$), this should be noted as a limitation.
Beyond reliability, reviewers should expect authors to explain conceptually why their human judgments should be valid, considering construct definitions, decision rules, and judge expertise.
As with other guidelines, missing information is typically a revision request. Absent judge instructions, instruments, decision rules, or construct definitions may prevent assessment of rigor and validity, while missing recruitment details are less critical. Clarification requests about construct definitions are routine and should not alone warrant rejection.