% \todo{Paul Ralph: Moving this guideline (use suitable baselines, etc.) above guideline 5 (Use human validation) would enable a cleaner presentation because we'd be able to explain about validating benchmarks here, then say "and if you don't have a validated benchmark then use humans".}

\guidelinesubsubsection{Rationale}

Meaningful evaluation requires well-understood, valid measurement instruments.
Without justified benchmarks and metrics, claims about LLM performance lack the rigor needed for scientific comparison and cumulative progress.
Benchmarks, baselines, and metrics play an important role in assessing the effectiveness of LLMs and LLM-based tools. A \textit{benchmark} is a ``standard tool for the competitive evaluation and comparison of competing systems or components according to specific characteristics, such as performance, dependability, or security''~\cite{Kistowski2015Benchmark}. Meanwhile, ``a \textit{metric} is a method, algorithm, or procedure for assigning one or more numbers to a phenomenon''~\cite{ralph2024teaching}. A baseline is a reference point, enabling comparison of LLMs against traditional algorithms with lower computational costs.

\guidelinesubsubsection{Recommendations}

When selecting benchmarks and metrics, it is important to fully understand the benchmark tasks, what exactly is being measured, and how it relates to the (often latent) variables researchers actually care about. If one or more metrics or benchmarks are used, researchers:
\begin{itemize}
    \item \must briefly justify (in the \paper) why selected benchmarks and metrics are suitable for the given task or study;
    \item \must discuss the reliability and validity (especially construct validity) of selected metrics and benchmarks;
    \item \should summarize the structure and tasks of the selected benchmark(s), including the programming language(s) and descriptive statistics such as the number of contained tasks and test cases;
    \item \should discuss the limitations of the selected benchmark(s) (e.g. widely used benchmarks such as \emph{HumanEval} \cite{DBLP:journals/corr/abs-2107-03374} and \emph{MBPP} \cite{DBLP:journals/corr/abs-2108-07732} Python functions assess a very specific part of software development, which is not representative of the full breadth of SE work~\cite{Chandra2025benchmarks}).
    \item \should include an example of a task and the corresponding test case(s) to illustrate the structure of the benchmark.
\end{itemize}

If multiple benchmarks exist for the same task, researchers \should compare performance between benchmarks.
When selecting only a subset of all available benchmarks, researchers \should use the most specific benchmarks given the context.

Furthermore, researchers \should check whether a less resource-intensive approach (e.g., for static analysis tasks or program repair) can serve as a baseline. If so, the LLM or LLM-based tool should be compared with such baselines using suitable metrics. Even if LLM-based tools outperform baselines, researchers \should discuss whether the resources consumed justify the (often marginal) improvements~\cite{DBLP:journals/cacm/Menzies25}.

To compare traditional and LLM-based approaches or different LLM-based tools, researchers \should report established metrics whenever possible, as this allows secondary research.
They can, of course, report additional metrics that they consider appropriate.
We briefly discuss common metrics in the Example(s) subsection below.
As mentioned, researchers \must motivate why they chose a certain metric or variant thereof for their particular study.

Due to LLM non-determinism, researchers \should repeat experiments and report descriptive statistics of model or tool performance (e.g., arithmetic mean, median, confidence intervals, standard deviations)~\cite{DBLP:conf/nips/AgarwalSCCB21, bjarnason2026randomnessagenticevals}. When comparing models or tools, researchers \should use appropriate inferential statistics (e.g., hypothesis tests such as the Mann-Whitney U test or bootstrap-based comparisons, along with effect sizes) rather than relying solely on differences in means or other summary statistics.

The number of required repetitions depends on factors such as the study type, the variability of the task, and the desired precision of estimates. As with sample sizes for human validation (\humanvalidation), researchers \should justify their chosen number of repetitions, for example, through a power analysis~\cite{Cohen1992, bjarnason2026randomnessagenticevals} or by monitoring the convergence of descriptive statistics across incremental runs~\cite{blackwell2024towards}. A pilot study can help estimate the expected variability and inform this decision.

From a measurement perspective, researchers \should reflect on the theories, values, and measurement models on which the benchmarks and metrics they have selected for their study are based.
For example, the phenomena labeled ``bugs'' in a large open dataset of software bugs are according to a certain theory of what constitutes a bug, and from the values and perspectives of the people who labeled the dataset. Reflecting on the context in which these labels were assigned and discussing whether and how the labels generalize to a new study context is crucial.

\guidelinesubsubsection{Example(s)}

\paragraph{Common Metrics:}

Two common metrics used for generation tasks are \emph{BLEU-N} and \emph{pass@k}.
\emph{BLEU-N}~\cite{DBLP:conf/acl/PapineniRWZ02} was originally developed for evaluating machine translation quality by measuring n-gram precision between a candidate and reference text, ranging from $0$ (dissimilar) to $1$ (similar). It has been widely adopted in SE for code generation tasks, though its validity in this context is debatable: n-gram overlap does not capture functional correctness, and syntactically different code can be semantically equivalent (see Challenges below).
Code-specific variations such as \emph{CodeBLEU}~\cite{DBLP:journals/corr/abs-2009-10297} and \emph{CrystalBLEU}~\cite{DBLP:conf/kbse/EghbaliP22} attempt to address these limitations by introducing additional heuristics such as AST matching.

The metric \emph{pass@k} reports the likelihood that a model correctly completes a code snippet at least once within $k$ attempts. Originally introduced as \emph{success rate at B} by \citeauthor{DBLP:journals/corr/abs-1906-04908}~\cite{DBLP:journals/corr/abs-1906-04908} and popularized by \citeauthor{DBLP:journals/corr/abs-2107-03374}~\cite{DBLP:journals/corr/abs-2107-03374}, it ranges from $0$ (no correct solution in $k$ tries) to $1$ (at least one correct solution), with correctness typically defined by passing test cases.

The \emph{pass@k} metric is defined as:
\[
\text{pass@}k = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\,, \text{where:}
\]
\begin{itemize}
  \item $n$ is the total number of generated samples per prompt,
  \item $c$ is the number of correct samples among $n$, and
  \item $k$ is the number of samples drawn.
\end{itemize}

The choice of $k$ depends on the downstream task: \emph{pass@1} is critical for single-suggestion scenarios like code completion, while higher $k$ values (e.g., $2$, $5$, $10$) assess multi-attempt capability~\cite{DBLP:journals/corr/abs-2308-12950, DBLP:journals/corr/abs-2401-14196, DBLP:journals/corr/abs-2409-12186, DBLP:journals/corr/abs-2305-06161}.
However, \emph{pass@k} is not a universal metric suitable for all generation tasks.
It requires a binary notion of correctness, making the metric appropriate for code synthesis evaluated via unit tests, but unsuitable for open-ended generation tasks such as comment generation, where multiple valid outputs exist.

If a study evaluates an LLM-based tool for supporting humans, a relevant metric is the acceptance rate, meaning the ratio of all accepted artifacts (e.g., test cases, code snippets) in relation to all artifacts that were generated and presented to the user.
Another way of evaluating LLM-based tools is calculating inter-model agreement.
This allows researchers to assess how dependent a tool's performance is on specific models and versions.
Metrics used to measure inter-model agreements include general agreement (percentage), \emph{Cohen's kappa}, and \emph{Krippendorff's $\alpha$} (see \humanvalidation for recommended thresholds and best practices for measuring agreement).

Common problem types for LLM-based studies are classification, recommendation, and generation, each requiring different metrics~\cite{10.1145/3695988}. \citeauthor{hu2025assessingadvancingbenchmarksevaluating}~\cite{hu2025assessingadvancingbenchmarksevaluating} categorized 191 LLM benchmarks by SE task, providing a valuable reference. Common metrics include \emph{BLEU}, \emph{pass@k}, \emph{Accuracy@k}, and \emph{Exact Match} for generation; \emph{Mean Reciprocal Rank} for recommendation; and \emph{Precision}, \emph{Recall}, \emph{F1-score}, and \emph{Accuracy} for classification.

\paragraph{Benchmark Examples:}

Benchmarks used for code generation include \emph{HumanEval} (available on \href{https://github.com/openai/human-eval}{GitHub}) \cite{DBLP:journals/corr/abs-2107-03374}, \emph{MBPP} (available on \href{https://huggingface.co/datasets/google-research-datasets/mbpp}{Hugging Face}) \cite{DBLP:journals/corr/abs-2108-07732},
% Both benchmarks consist of code snippets written in Python sourced from publicly available repositories.
% Each snippet consists of four parts: a prompt containing a function definition and a corresponding description of what the function should accomplish, a canonical solution, an entry point for execution, and test cases.
% The input of the LLM is the entire prompt.
% The output of the LLM is then evaluated against the canonical solution using metrics or against a test suite.
\emph{ClassEval} (available on \href{https://github.com/FudanSELab/ClassEval}{GitHub}) \cite{DBLP:journals/corr/abs-2308-01861}, \emph{LiveCodeBench} (available on \href{https://github.com/LiveCodeBench/LiveCodeBench}{GitHub}) \cite{DBLP:journals/corr/abs-2403-07974}, and \emph{SWE-bench} (available on \href{https://github.com/swe-bench/SWE-bench}{GitHub}) \cite{DBLP:conf/iclr/JimenezYWYPPN24}.
An example of a code translation benchmark is \emph{TransCoder}~\cite{DBLP:journals/corr/abs-2006-03511} (available on \href{https://github.com/facebookresearch/CodeGen}{GitHub}). 

Most benchmarks focus on generation tasks, but benchmarks for classification and recommendation also exist.
For classification, \emph{DiverseVul} (available on \href{https://github.com/wagner-group/diversevul}{GitHub})~\cite{DBLP:conf/raid/0001DACW23} provides vulnerable and non-vulnerable functions evaluated using standard classification metrics.
For recommendation, \emph{CodeSearchNet} (available on \href{https://github.com/github/CodeSearchNet}{GitHub})~\cite{DBLP:journals/corr/abs-1909-09436} contains code-documentation pairs evaluated using \emph{Mean Reciprocal Rank}.

\guidelinesubsubsection{Benefits}

Benchmarks and metrics improve reproducibility and comparability across studies, leading to faster improvements in the cumulative body of knowledge. It is simply easier to assess a new system against one or a few benchmarks than hundreds of competing systems, and when most systems are assessed against the same benchmarks and metrics, we can see which approaches work best (at least, on those benchmarks). This comparison enables progress tracking; for example, when researchers iteratively improve a new LLM-based tool and test it against benchmarks after significant changes. For practitioners, leaderboards (published benchmark results of models) support selecting models for downstream tasks. Meanwhile, baselines help us understand just how much LLMs improve performance over non-LLM-enabled alternatives. 

\guidelinesubsubsection{Challenges}

Computer scientists have a long history of using oversimplified, unvalidated metrics~\cite{DBLP:conf/ease/RalphT18} (e.g., lines of code, CPU time) as proxies for complex, multidimensional latent variables (e.g., system size, environmental sustainability). In fields such as psychology, where measurement theory has a longer tradition~\cite{borsboom2005measuring}, metrics are backed by foundational theories or extensive empirical validation of their psychometric properties. The fundamental challenge with AI metrics and benchmarks is that many have neither firmly understood theoretical underpinnings nor extensive empirical validation of their construct, measurement, and ecological validity.

For example, \emph{pass@k} is intended to measure a model's \textit{functional correctness}, that is, its ability to generate code that produces the expected output for a given specification. However, functional correctness is only one dimension of code quality. \emph{pass@k} does not capture maintainability, readability, security, or efficiency of the generated code, all of which are critical for downstream use. Furthermore, ``correctness'' is defined entirely by test suites, whose coverage is itself unvalidated: a solution that passes all provided tests may still be incorrect for untested inputs. Whether a test suite adequately operationalizes correctness for a given task is a subjective judgment that is rarely examined.

Similarly, \emph{HumanEval} is intended to measure a model's ability to \textit{synthesize short Python functions from docstring specifications}. This operationalizes a narrow slice of ``code generation ability'': it covers neither multi-file tasks, nor debugging, nor the use of existing codebases---tasks that dominate real-world software engineering~\cite{Chandra2025benchmarks}. Notably, neither \emph{pass@k} nor \emph{HumanEval} has undergone rigorous empirical validation of its construct validity; their widespread adoption rests on face validity and convenience rather than on evidence that they reliably measure what researchers intend them to measure~\cite{cao2025should}.

Benchmarks and metrics also have generalizability problems. Prominent LLM benchmarks such as \emph{HumanEval} and \emph{MBPP} use Python, so researchers can optimize for Python's idiosyncrasies, creating illusory improvements that do not generalize to other languages. Similarly, the metric \emph{BLEU-N} is a syntactic metric, so code can score highly without being executable. The metric \emph{Exact Match}, meanwhile, does not account for functional equivalence of syntactically different code. Both \emph{BLEU-N} and \emph{Exact Match} are influenced by code formatting, which confounds their intended use.

Execution-based metrics such as \emph{pass@k} directly evaluate correctness by running test cases, but they require a setup with an execution environment.
When researchers observe unexpected values for certain metrics, the specific results should be investigated in more detail to uncover potential problems.

% Many closed-source models, such as those released by \emph{OpenAI}, achieve exceptional performance on certain tasks but lack transparency and reproducibility~\cite{DBLP:conf/nips/00110ZZDJLHL24, DBLP:journals/corr/abs-2308-01861, DBLP:journals/corr/abs-2406-15877}.
% Benchmark leaderboards, particularly for code generation, are led by closed-source models~\cite{DBLP:journals/corr/abs-2308-01861, DBLP:journals/corr/abs-2406-15877}.
% While researchers \should compare performance against these models, they must consider that providers might discontinue them or apply undisclosed pre- or post-processing beyond the researchers' control (see \openllm).
%The lack of transparency introduces challenges in analyzing mistakes, limiting the understanding of model failures.
%Open-source models offer greater transparency since the entire process is under the researcher's control; however, they require appropriate hardware.

Finally, benchmark data contamination---where the benchmark itself is part of the training data---may lead to artificially high performance if the model remembers the solution from the training data rather than actually solving the task based on the input data~\cite{DBLP:journals/corr/abs-2406-04244}. Therefore, for proprietary LLMs that do not release their training data, researchers \should consider using human validation, curating new data, or refactoring existing data~\cite{DBLP:journals/corr/abs-2406-04244}. 

To mitigate contamination, researchers can create new benchmark datasets by collecting data after a specified cutoff date; researchers \must disclose data sources and collection dates for each release. However, this temporal approach requires continuous updates as new models may include the benchmark in future training data. Alternatively, keeping the benchmark private prevents inclusion in training sets, but requires trust in the benchmark creator and a system to execute it without leaking data. Researchers can also evaluate how contaminated their benchmark is~\cite{choi2025contaminatedbenchmarkquantifyingdataset}.


\guidelinesubsubsection{Study Types}

This guideline \must be followed for all study types that automatically evaluate the performance of LLMs or LLM-based tools.
The design of a benchmark and the selection of appropriate metrics are highly dependent on the specific study type and research goal.
Recommending specific metrics for specific study types is beyond the scope of these guidelines, but \citeauthor{hu2025assessingadvancingbenchmarksevaluating} provide a good overview of existing metrics for evaluating LLMs~\cite{hu2025assessingadvancingbenchmarksevaluating}.

For \benchmarkingtasks, this guideline is of primary importance: researchers \must use established benchmarks or rigorously justify the creation of new ones, and \must report standard metrics to enable cross-study comparison.
For \annotators, the research goal might be to assess which model comes close to a ground truth dataset created by human annotators.
Especially for open annotation tasks, selecting suitable metrics to compare LLM-generated and human-generated labels is important.
In general, annotation tasks can vary significantly.
Are multiple labels allowed for the same sequence? Are the available labels predefined, or should the LLM generate a set of labels independently?
Due to this task dependence, researchers \must justify their metric choice, explaining what aspects of the task it captures together with known limitations.
For \newtools, researchers \should benchmark the tool against suitable baselines using appropriate metrics that capture the tool's intended contribution.
For \judges, researchers \should report inter-rater agreement metrics and validity measures to demonstrate the reliability and quality of LLM judgments.
For \synthesis, researchers \should specify metrics for comparing synthesized outputs (e.g., coverage, faithfulness) and justify their appropriateness for the synthesis task.
For \llmusage, researchers \should justify the measurement instruments and metrics used for studying LLM usage patterns, including any survey scales or behavioral measures.
For \subjects, researchers \should compare simulated and real human responses using appropriate metrics to assess simulation fidelity.
If researchers assess a well-established task such as code generation, they \should report standard metrics such as \emph{pass@k} and compare the performance between models.
If non-standard metrics are used, researchers \must state their reasoning.

%% Changed: Softened "must not tolerate" → "should not accept" (advisory tone).
%% Changed: Simplified "ambivalence toward construct validity or obfuscation of validity concerns" → "dismissive or evasive treatment of construct validity".
%% Suggestion: Consider breaking the closing paragraph into shorter sentences for readability.
\guidelinesubsubsection{Advice for Reviewers}

Reviewers should expect manuscripts to:
\begin{enumerate*}
    \item clearly identify the constructs or variables the study aims to measure (e.g., LLM performance, quality of generated code), including independent, dependent, and control variables;
    \item present their measurement model, i.e., which metrics, benchmarks, or baselines are used and how they relate to the target constructs;
    \item justify the selection of any metrics, benchmarks, and baselines used;
    \item discuss \textit{in detail} the assumptions, reliability, and validity---\textit{especially construct validity}---of each benchmark and metric;
    \item articulate any limitations regarding construct and measurement validity.
\end{enumerate*}
As with other guidelines, missing information about baselines or metrics is typically a revision request. However, vague descriptions that conflate broad concerns (e.g., effectiveness, quality) with specific counting methods should be questioned. Ubiquity of a benchmark does not imply validity or appropriateness for a given context. Manuscripts should convey a solid understanding of construct and measurement validity by explaining and justifying their measurement models.

\guidelinesubsubsection{See Also}
\begin{itemize}[label=$\rightarrow$]
    \item Section~\openllm: open models for inter-model comparison.
    \item Section~\humanvalidation: human evaluation to complement automated metrics.
    \item Section~\limitationsmitigations: reporting construct and measurement validity limitations.
\end{itemize}