\input{../../header.tex}

\begin{document}

\subsection{Report Limitations and Mitigations}

\subsubsection{Recommendations}

When using LLMs for empirical studies in software engineering, researchers face unique challenges and potential limitations that can influence the validity, reliability, and reproducibility of their findings~\cite{sallou2024breaking}.
 including:

A cornerstone of open science is the ability to reproduce research results.
Even though the inherent non-deterministic nature of LLMs is a strength in many use cases, its impact on \emph{reproducibility} is a challenge. To enable reproducibility, researchers \must disclose a replication package for their study.
\todo{Sebastian: This is not possible for all study types. If we keep it that generic, it's rather a \should. Or we contextualize by study type.} 
 They \should perform multiple evaluation repetitions of their experiments (see \href{/guidelines/report-baselines-benchmarks-and-metrics}{Report Suitable Baselines, Benchmarks, and Metrics}) to account for non-deterministic outputs of LLMs or
In same cases, they \may reduce output variability by setting the temperature to a value close to 0 and configure a seed value for more deterministic results.
However, configuring a lower temperature can impair task performance, and not all models allow configuring seed values.
Besides non-determinism, the behavior of an LLM depends on many external factors, such as model version, API updates, or prompt variations.
To ensure reproducibility, researchers \should follow all suggestions in our guidelines and further address the generalization challenges outlined below.

Even though the topic of \emph{generalizability} is not new, it has gained new relevance with the increasing interest in LLMs. In LLM-based studies, generalizability boils down to two main concerns:
First, are the results specific to an LLM, or can they be achieved with other LLMs too? 
If generalizability to other LLMs is not in the scope of the research, this \must be clearly explained in the \paper or, if generalizability is in scope, researchers \must compare their results or subsets of the results (if not possible e.g., due to computational cost) with other LLMs that are similar (e.g. in size) to assess the generalizability of their findings (see also Section \openllm).
Second, will these results still be valid in the future?
Multiple studies (e.g., \cite{DBLP:journals/corr/abs-2307-09009, doi:10.1148/radiol.232411}) found that the performance of proprietary LLMs (e.g., GPT) within the same version (e.g. GPT-4o) decreased over time for certain tasks.
Reporting the model version and configuration is not sufficient in such cases. To date, the only way of mitigating this limitation is the usage of an Open LLM with a transparently communicated versioning schema and archiving (see \openllm). Hence, researchers \should employ open LLMs to set a reproducible baseline and, if employing an open LLM is not possible, researchers \should test and report their results over an extended period of time as a proxy of the results' stability over time.

\emph{Data leakage/contamination/overfitting} occurs when information from outside the training dataset influences the model, leading to overly optimistic performance estimates.
With the growing reliance on big datasets, the risks of inter-dataset duplication increases (see, e.g.,~\cite{DBLP:journals/pacmpl/LopesMMSYZSV17, DBLP:conf/oopsla/Allamanis19}). 
In the context of LLMs for software engineering, this can, for example, manifest as samples from the pre-train dataset appearing in the fine-tune or evaluation dataset, potentially compromising the validity of evaluation results~\cite{DBLP:journals/tse/LopezCSSV25}.
For example ChatGPT's functionality to ``improve the model for everyone'' can result in unintentional data leakage.
Hence, to ensure the validity of the evaluation results, researchers \should carefully curate the fine-tuning and evaluation datasets to prevent inter-dataset duplication and \mustnot leak their fine-tuned or evaluation datasets into the improvement process of the LLM.
However, when publishing the results, researchers \should of course still follow open science practices and publish the datasets as part of their \supplementarymaterial.
If information about the pre-train dataset of the employed LLM is available, researchers \should assess the inter-dataset duplication and \must discuss the potential data leakage in the \paper.
When training an LLM from scratch, researchers \may consider using open datasets such as Together AI's RedPajama~\cite{together2023redpajama} that already incorporate duplication (with the positive side effect of potentially improving performance~\cite{DBLP:conf/acl/LeeINZECC22}).
\todo{Sebastian: Is incorporate the right word here? Sounds like the dataset contains duplicates.}

Conducting studies with LLMs is a resource-intensive endeavor.
For self-hosted LLMs, the respective hardware needs to be provided, for managed LLMs, the service \emph{cost} has to be considered.
The challenge becomes more pronounced as LLMs grow larger, research architectures get more complex, and experiments become more computationally expensive.
For example, multiple repetitions to assess performance in the face of non-determinism (see \href{/guidelines/report-baselines-benchmarks-and-metrics}{Report Suitable Baselines, Benchmarks, and Metrics}) multiplies the cost and impacts \emph{scalability}.
Consequently, resource-intensive research remains predominantly the domain of private companies or well-funded research institutions, hindering researchers with limited resources in reproducing, replicating, or extending study results.
Hence, for transparency reasons, researchers \should report the cost associated with executing an LLM-based study. 
If the study employed self-hosted LLMs, researchers \should report the specific hardware used. 
If the study employed managed LLMs, the service cost \should be reported.
To ensure research result validity and reproducibility, researchers \must provide the LLM outputs as evidence for validation (see Section~\prompts).
Depending on the architecture (see Section~\toolarchitecture), these outputs need to be reported on different architectural levels (e.g., outputs of individual LLMs in multi-agent systems).
\todo{Sebastian: ``a subset of they employed validation dataset''?}
Additionally, researchers \should include a subset of they employed validation dataset, selected using an accepted sampling strategy, to allow partial replication of the results.

While \emph{metrics} such as BLEU or ROUGE are commonly used to evaluate the performance of LLMs (see Section~\benchmarksmetrics), they may not capture other relevant, software engineering-specific aspects such as functional correctness or the runtime performance of automatically generated code~\cite{DBLP:conf/nips/LiuXW023}.
%Researchers \should clarify and state all relevant requirements, and employ and report the metrics to measure the satisfaction of the requirements (e.g., test-case success rate) and \should follow the best practices described in \href{/guidelines/report-baselines-benchmarks-and-metrics}{Report Suitable Baselines, Benchmarks, and Metrics}.

\emph{Sensitive data} can range from personal to proprietary data, each with its own set of \emph{ethical concerns}.
A big threat of proprietary LLMs and sensitive data is the data's usage for model improvements (see discussion of data leakage above).
Hence, using sensitive data can lead to privacy and intellectual property (IP) violations.
Another threat is the implicit bias of LLMs potentially leading to discrimination or unfair treatment of individuals or groups.
To mitigate these concerns, researchers \should minimize the sensitive data used in their studies, \must follow applicable regulations (e.g., GDPR) and individual processing agreements, \should create a data management plan outlining how the data is handled and protected against leakage and discrimination, and \must apply for approval from the ethics committee of their organization (if required).

%\enquote{The field of AI is currently primarily driven by research that seeks to maximize model accuracy â€” progress is often used synonymously with improved prediction quality. This endless pursuit of higher accuracy over the decade of AI research has significant implications for computational resource requirements and environmental footprint. To develop AI technologies responsibly, we must achieve competitive model accuracy at a fixed or even reduced computational and environmental cost.}~\cite{DBLP:conf/mlsys/WuRGAAMCBHBGGOM22}

The performance of an LLM is usually measured in terms of traditional metrics such as accuracy, precision, and recall or more contemporary metrics such as pass@k, or BLEU-N (see Section \benchmarksmetrics).
However, given the high resource demand of LLMs are, \emph{resource consumption} has to become a key indicator for performance to assess research progress responsibly. 
While research predominantly focused on the energy consumption during the early phases of LLMs (e.g., data center manufacturing, data acquisition, training), inference - i.e. the use of the LLM - often becomes similarly or even more resource-intensive~\cite{de2023growing, DBLP:conf/mlsys/WuRGAAMCBHBGGOM22, DBLP:journals/corr/abs-2410-02950, JIANG2024202, mitu2024hidden}.
Hence, researchers  \should aim for lower resource consumption on the model side.
This can be achieved by selecting smaller (e.g., GPT 4o mini instead of GPT 4o) or newer models as a base model for the study or by employing techniques such as model pruning, quantization, knowledge distillation~\cite{mitu2024hidden}.
They \should further reduce resource consumption when using the LLMs, e.g. by restricting the number of queries, input tokens, or output tokens~\cite{mitu2024hidden}, with different prompt engineering techniques (e.g., on average zero-shot prompts seem to emit less CO2 than chain-of-thought prompts), or by carefully sampling smaller datasets for fine-tuning and evaluation instead of using large datasets in their entirety.
\todo{Sebastian: This of course goes against our recommendations to repeat studies. How do we balance this?}
To report the environmental impact of a study, researchers \should use software such as \href{https://github.com/mlco2/codecarbon}{CodeCarbon} or \href{experiment-impact-tracker}{Experiment Impact Tracker} to track and quantify the carbon footprint of the study or report an estimation of the carbon footprint through tools such as \href{https://mlco2.github.io/impact/#about}{MLCO2 Impact}.
They \should detail the LLM version and configuration as described in \modelversion, state the hardware or hosting provider of the model as described in \toolarchitecture and report the total number of requests, accumulated input tokens, and output tokens.
Researchers \must justify why an LLM was chosen over existing approaches and set the achieved results in relation to the higher resource consumption of LLMs (see also Section \benchmarksmetrics).


\subsubsection{Example(s)}

An example highlighting the need for caution around \emph{replicability} is the study of Staudinger et al.~\cite{DBLP:conf/sigir-ap/StaudingerKPLH24} who attempted the replication of an LLM study. They aimed to replicate the results of a previous study that did not provide a replication package. However, they were not able to reproduce the exact results, even though they saw similar trends to the original study. They consider their results as not reliable enough for a systematic review.
To analyze whether the results of proprietary LLMs transfer to open LLMs, Staudinger et al.~\cite{DBLP:conf/sigir-ap/StaudingerKPLH24} benchmarked previous results using GPT3.5 and GPT4 against Mistral and Zephyr. They found that the employed open-source models could not deliver the same performance as the proprietary models, restricting the effect to certain proprietary models.
This paper is also an example of a study reporting the \emph{costs}: \enquote{120 USD in API calls for GPT 3.5 and GPT 4, and 30 USD in API calls for Mistral AI. Thus, the total LLM cost of our reproducibility study was 150 USD}.

Individual studies already started to highlight the uncertainty about the \emph{generalizability} of their results in the future. In~\cite{DBLP:conf/msr/JesseADM23} Jesse et al. acknowledge the issue that LLMs evolve over time and that this evolution might impact the study's results.
Since a lot of research in software engineering evolves around code, inter-dataset code duplication has been extensively researched and addressed over the years to curate deduplicated datasets (see, e.g., \cite{DBLP:journals/pacmpl/LopesMMSYZSV17, DBLP:conf/oopsla/Allamanis19, DBLP:journals/ese/KarmakarAR23, DBLP:journals/tse/LopezCSSV25}).
The issue of inter-dataset duplication has also attracted interest in other disciplines, with growing demands for data mining.
For example, in the biology field, Lakiotaki et al.~\cite{DBLP:journals/biodb/LakiotakiVTGT18} acknowledge and address the overlap between multiple common disease datasets. 
In the domain of code generation, Coignion et al.~\cite{DBLP:conf/ease/CoignionQR24} evaluated the performance of LLMs to produce leet code.
To mitigate the issue of inter-dataset duplication, they only used leet code problems published after 2023-01-01, reducing the likelihood of LLMs having seen those problems before. Further, they discuss the performance differences of LLMs on different datasets in light of potential inter-dataset duplication.
Zhou et al. performed an empirical evaluation of data leakage in 83 software engineering benchmarks~\cite{zhou2025lessleakbenchinvestigationdataleakage}.
While most benchmarks suffer from minimal leakage, very few suffered from leakage of up to 100\%. They found a high impact of data leakage on the performance evaluation.
A starting point for studies that aim to assess and mitigate inter-dataset duplication are the \href{https://huggingface.co/datasets/tiiuae/falcon-refinedweb}{Falcon LLMs}.
The technology innovation institute publicly provides access to parts of its training data for the Falcon LLMs via Hugging Face~\cite{technology_innovation_institute_2023}.
Through this dataset, it is possible to reduce the overlap between the pre-train and evaluation data, improving the validity of the evaluation results.
A starting point to prevent actively leaking data into a LLM improvement process is to ensure that the data is not used to train the model (e.g., via OpenAI's data control functionality, or the OpenAI API instead of the web interface)~\cite{DBLP:conf/eacl/BalloccuSLD24}.

Bias can occur in datasets as well as in LLMs that have been trained on them and may result in various types of discrimination.
Gallegos et al. propose metrics to quantify biases in various tasks (e.g., text generation, classification, question answering)~\cite{DBLP:journals/corr/abs-2309-00770}.
Tinnes et al.~\cite{tinnessoftware} balanced the dataset size between the need for manual semantic analysis and computational resource consumption.


\subsubsection{Advantages}

%Ensuring \emph{reproducibility} allows for the independent replication and verification of study results.
\emph{Reproducing} study results under similar conditions by different parties greatly increases their validity and promotes transparency in research.
Independent verification is of particular importance in studies involving LLMs, due to the randomness of their outputs and the potential for biases in their predictions and in training, fine-tuning, and evaluation datasets.
Mitigating threats to \emph{generalizability} of a study through the integration of open LLMs as a baseline or the reporting of results over an extended period of time can increase the validity, reliability, and replicability of a study's results.
Assessing and mitigating the effects of \emph{inter-dataset duplication} strengthens a study's validity and reliability, as it prevents overly optimistic performance estimates that do not apply to previously unknown samples.
Reporting the \emph{costs} associated with executing a study not only increases transparency but also supports secondary literature in setting primary research into perspective.
Providing \emph{replication packages} entailing direct LLM output evidence as well as samples for partial replicability are paramount steps towards open and inclusive research in the light of resource inequality among researchers.
Mindfully deciding and justifying the usage of LLMs over other approaches can lead to more efficient and sustainable approaches. 
Reporting the \emph{environmental impact} of the usage of LLMs also sets the stage for more sustainable research practices in the field of AI.


\subsubsection{Challenges}

With commercial LLMs evolving over time, the \emph{generalizability} of results to future versions of the model is uncertain.
Employing open LLMs as a baseline can mitigate this limitation, but may not always be feasible due to computational cost.
Most LLM providers do not publicly offer information about the datasets employed for pre-training, impeding the assessment of \emph{inter-dataset duplication} effects.
Consistently keeping track of and reporting the \emph{costs} involved in a research endeavor is challenging.
Building a coherent \emph{replication package} that includes LLM outputs and samples for partial replicability requires additional effort and resources.
Defining all requirements for metrics beforehand to ensure the usage of suitable \emph{metrics} can be challenging, especially in exploratory research.
In this growing field of research, finding the right metrics to evaluate the performance of LLMs in software engineering for specific tasks is challenging.
Our Section \benchmarksmetrics and its references can serve as a starting point.
Ensuring \emph{compliance} across jurisdictions is difficult with different regions having different regulations and requirements (e.g., GDPR and the AI Act in the EU, CCPA in California).
Selecting datasets and models with fewer \emph{biases} is challenging, as the bias in LLMs is often not transparently reported.
Measuring or estimating the \emph{environmental impact} of a study is challenging and might not always be feasible.
Especially in exploratory research, the impact is hard to estimate beforehand, making it difficult to justify the usage of LLMs over other approaches.


\subsubsection{Study Types}

The limitations and mitigations \should be followed for all study types in a sensible manner, i.e. depending on the applicability to the individual study.


\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}
