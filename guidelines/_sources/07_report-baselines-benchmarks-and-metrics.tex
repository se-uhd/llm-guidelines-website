\input{../../header.tex}

\begin{document}

\subsection{Report Suitable Baselines, Benchmarks, and Metrics}

\subsubsection{Recommendations}

\begin{quote}
\underline{tl;dr:} Researchers \must justify all benchmark and metric choices in the \paper and \should summarize benchmark structure, task types, and limitations. Where possible, traditional (non-LLM) baselines \should be used for comparison. Researchers \must explain why the selected metrics are suitable for the specific study. They \should report established metrics to make study results comparable, but can report additional metrics that they consider appropriate. Due the inherent non-determinism of LLMs, experiments \should be repeated; the result distribution \should then be reported using descriptive statistics.
\end{quote}

Benchmarks, baselines, and metrics play a significant role in assessing the effectiveness of an LLM or LLM-based tool.
Benchmarks are model- and tool-independent standardized tests used to assess the performance of LLMs on specific tasks such as code summarization or code generation.
A benchmark consists of multiple standardized test cases, each with at least a task and an expected result.
Metrics are used to quantify the performance for the benchmark tasks, enabling a comparison.
A baseline represents a reference point for the measured LLM.
Since LLMs require substantial hardware resources, baselines serve as a comparison to assess their performance against traditional algorithms with lower computational costs.

When selecting benchmarks, it is important to fully understand the benchmark tasks and the expected result, because they determine what the benchmark actually assesses.
Researchers \must briefly summarize why they selected certain benchmarks in the \paper, 
They \should also summarize the structure and tasks of the selected benchmark(s), including the programming language(s) and descriptive statistics such as the number of contained tasks and test cases.
Researchers \should also discuss the limitations of the selected benchmark(s).
For example, many benchmarks focus heavily on Python, and often on isolated functions.
This assesses a very specific part of software development, which is certainly not representative for the full breadth of software engineering.

Researchers \may include an example of a task and corresponding test case(s) the illustrate the structure of the benchmark.
If multiple benchmark exist for the same task, researchers \should compare performance between benchmarks.
If selecting only a subset of benchmarks, researchers \should use of the most specific benchmarks given the context.

The use of LLMs might not always be reasonable if traditional approaches achieve similar performance. 
For many tasks LLMs are being evaluated for, there exist traditional non-LLM-based approaches (e.g., for program repair) that can serve as a baseline.
Even if LLM-based tools perform better, the question is whether the resources consumed justify the potentially marginal improvements.
Researchers \should always check whether such traditional baselines exist and if they do, compare them with the LLM or LLM-based tool using suitable metrics.

For comparing traditional and LLM-based approaches or different LLM-based tools, researchers \should report established metrics whenever possible as this enables secondary research, but they can report additional metrics that they consider appropriate.
Researchers \must argue why the selected metrics are suitable for the given task or study. 
If a study evaluates an LLM-based tool that is supposed to support humans, a relevant metric is the acceptance rate, meaning the ratio of all accepted artifacts (e.g., test cases, code snippets) in relation to all artifacts that were generated and presented to the user.
Another way of evaluating LLM-based tools is calculating inter-model agreement (see also Section \href{/guidelines/#use-an-open-llm-as-a-baseline}{Use an Open LLM as a Baseline}).
This allows researchers to assess how dependent a tool's performance is on specific models.
Metrics used to measure inter-model agreements are percent agreement, Cohen's kappa and Scott's Pi coefficient.

LLM-based generation is non-deterministic by-design.
Due to this non-determinism, researchers \should repeat experiments to statistically assess the performance of a model or tool, for example using the arithmetic mean, confidence intervals, and standard deviations.

From a measurement perspective, researchers \should reflect on the theories, values, and measurement models on which the benchmarks and metrics they have selected for their study depend.
For example, a large open dataset of software bugs is a collection of bugs according to a certain theory of bugs, and the values and perspective of the people who labelled the dataset. Reflecting on the context in which these labels were assigned and discussing if and how the labels generalize to a new study context is crucial.


\subsubsection{Example(s)}

Hu et al.~\citet{hu2025assessingadvancingbenchmarksevaluating} conducted a comprehensive structured literature review on LLM benchmarks related to software engineering tasks.
They analyzed 191 benchmarks and categorized them based on the specific task they address, making the paper a valuable resource for identifying relevant existing benchmarks.

Two benchmarks used for code generation are \emph{HumanEval} (\href{https://github.com/openai/human-eval}{GitHub}) \cite{DBLP:conf/acl/PapineniRWZ02} and \emph{MBPP} (\href{https://huggingface.co/datasets/google-research-datasets/mbpp}{GitHub}) \cite{DBLP:journals/corr/abs-2108-07732}.
Both benchmarks consist of code snippets written in Python sourced from publicly available repositories.
Each snippet consists of four parts: a prompt based on function definition and a corresponding description what the function should accomplish, a canonical solution, an entry point for execution, and tests.
The input of the LLM is the entire prompt.
The output of the LLM is evaluated either against the canonical solution using metrics or against a test suite.
Other benchmarks for code generation include \emph{ClassEval} (\href{https://github.com/openai/human-eval}{GitHub}) \cite{DBLP:journals/corr/abs-2308-01861}, \emph{LiveCodeBench} (\href{https://github.com/LiveCodeBench/LiveCodeBench}{GitHub}) \cite{DBLP:journals/corr/abs-2403-07974}, and \emph{SWE-bench} (\href{https://github.com/swe-bench/SWE-bench}{GitHub}) \cite{DBLP:conf/iclr/JimenezYWYPPN24}.
An example of a code translation benchmark is \emph{TransCoder}~\cite{DBLP:journals/corr/abs-2006-03511} (\href{https://github.com/facebookresearch/CodeGen}{GitHub}). 

According to \citet{10.1145/3695988}, main problems types for LLMs are classification, recommendation and generation problems.
Each of these problem types requires a different set of metrics.
They provide a comprehensive overview of benchmarks categorized by software engineering tasks.
Common metrics for assessing generation tasks are \emph{BLEU}, \emph{pass@k}, \emph{Accuracy/ Accuracy@k}, and \emph{Exact Match}~\cite{10.1145/3695988}.
The most common recommendation task metric is \emph{Mean Reciprocal Rank}~\cite{10.1145/3695988}.
For classification tasks, classical machine learning metrics such as \emph{Precision}, \emph{Recall}, \emph{F1-score}, and \emph{Accuracy} are often reported \cite{10.1145/3695988}.

Now, we briefly discuss two common metrics used for generation tasks.
\emph{BLEU-N} \cite{DBLP:conf/acl/PapineniRWZ02} is a similarity score based on n-gram precision between two strings, ranging from $0$ to $1$.
Values close to $0$ depict dissimilar values closer to $1$ represent similar content.
A value closer to $1$ indicates that the model is more capable of generating the expected output for code generation.
%The score measures how many n-grams from the generated output of a model appear in the target string.
%In isolation, this approach favors shorter over longer generated sequences because it is more likely to have fewer n-grams in an extensive target sequence than more.
%To mitigate this, the so-called brevity penalty is introduced to disincentivize short sequences when having longer target sequences.
\emph{BLEU-N} has multiple variations.
\emph{CodeBLEU} \cite{DBLP:journals/corr/abs-2009-10297} and \emph{CrystalBLEU} \cite{DBLP:conf/kbse/EghbaliP22} are the most notable variations tailored to code, by introducing additional heuristics such as AST matching.
As mentioned above, researchers should motivate why they chose a certain metric or variant thereof for their particular study.

The metric \emph{pass@k} reports the likelihood of a model correctly completing a code snippet at least once within \emph{k} tries.
To the best of our knowledge, the basic concept of pass@k was first used in \cite{DBLP:journals/corr/abs-1906-04908} for evaluating code synthesis under the name \emph{success rate at B}, where B denotes the budget of trials.
The term pass@k was later popularized by \cite{DBLP:journals/corr/abs-2107-03374} as a metric for code generation correctness.
The exact definition of correctness varies depending on the task.
For code generation, correctness is often defined based on test cases. A passing test then means that the solution is correct.
The resulting pass rate ranges from $0$ to $1$.
A pass rate of $0$ indicates that the model was not able to generate a single correct solution within $k$ tries.
A pass rate of $1$ indicates that the model successfully generated at least one correct solution in $k$ tries.
The metric is defined as:

$\text{pass@k} = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}$

Where $n$ is the total number of generated samples per prompt, $c$ is the number of correct samples among  $n$, and $k$ is the number of samples.

Choosing an appropriate value for \emph{k} depends on the downstream task of the model and how end-users interact with the model.
% k= 1 -- pass@1
A high pass rate for \emph{pass@1} is highly desirable in tasks where the system only presents one solution or if a single solution requires high computational effort.
For example, code completion depends on a single prediction since the end user typically sees only a single suggestion.
%In automated code refactoring, a large context might lead to high computational costs, making the calculation of a single solution expensive.
%Thus, having a high pass rate at pass@1 is crucial for such tasks.
% k = >=2 -- pass@{>=2}
Pass rates for higher $k$ values (e.g., $2$, $5$, $10$) indicate whether the model can solve the given task within multiple attempts.
For downstream tasks that permit multiple solutions or user interaction, strong performance at k > 1 can be justified. 
For instance, a user selecting the correct test case from multiple suggestions allows for some model errors.
%In code search, finding the most appropriate good snippet for a given context, a reranking algorithm mitigate low pass rates at low k-values.
Common examples for papers using pass@k are papers introducing new models for code generation such as \cite{DBLP:journals/corr/abs-2308-12950, DBLP:journals/corr/abs-2401-14196, DBLP:journals/corr/abs-2409-12186, DBLP:journals/corr/abs-2305-06161}.
%\subsubsection{Exact Match}

\emph{Exact match} is another metric that calculates the percentage of replicas a model can produce.
If the model is able to produce the exact target sequence a score of 1 is awarded; otherwise 0.
Compared to BLEU-N, \emph{exact match} is a stricter measurement.
%Due to its strictness, reported scores are usually low.

The choice of metrics strongly depends on the specific task.
Consequently, papers addressing different research questions employ different metrics.
For example, Liu et al. assesses LLM models for their correctness on code generation using pass@k~\cite{DBLP:conf/nips/LiuXW023}.
However, pass@k is not an universal metric suitable for every problem at hand.
For instance, for creative or open-ended tasks such as comment generation, pass@k is not a suitable metric since not only one single correct result exists.
Thus, Geng et al. evaluates LLMs for code comment generation using BLEU, ROUGE-L and METEOR as evaluation metrics~\cite{DBLP:conf/icse/GengWD00JML24}.
Liu et al. investigates the performance of LLMs on automated refactorings tasks and proposes a novel metric tailored to the unique characteristics of this problem \cite{DBLP:journals/ase/LiuJZNLL25}.

%Code snippets provided by benchmarks contain untrusted code, making sandboxing recommendable.
%To avoid potential security risks, execute untrusted code in an isolated environment, such as a virtual machine or containerization, instead of your own environment.


\subsubsection{Advantages}

Benchmarks are an essential tool for assessing prompts and models' performance for software engineering tasks.
Due to inherent variations of LLMs, both the choice of models and prompt can lead to substantial differences in performance.
Reproducible benchmarks measure and assess the performance of both prompts and models for a specific software engineering task, creating a tool for comparison.
The comparison enables progress tracking and steering, for example, during prompt engineering by iteratively comparing benchmark results after changes.
For practitioners, leaderboards, published benchmark results of models, enable a swift selection of models for downstream tasks.

\subsubsection{Challenges}

A general challenge with benchmarks for LLMs is that the most prominent ones, such as \emph{HumanEval} and \emph{MBPP}, use Python, introducing a bias towards this specific programming language and its idiosyncrasies.
Since model performance is measured against these benchmarks, researchers often optimize for them.
As a result, performance may degrade if programming languages other than Python are used.

Many closed-source models, such as those released by OpenAI, achieve exceptional performance on certain tasks but lack transparency and reproducibility~\cite{DBLP:conf/nips/00110ZZDJLHL24, DBLP:journals/corr/abs-2308-01861, DBLP:journals/corr/abs-2406-15877}.
Benchmark leaderboards, particularly for code generation, are led by close-sourced models~\cite{DBLP:journals/corr/abs-2308-01861, DBLP:journals/corr/abs-2406-15877}.
While researchers should compare performance against these models, they must consider that providers might discontinue them or apply undisclosed pre- or post-processing beyond the researcher's control (see also Section see also Section \href{/guidelines/#use-an-open-llm-as-a-baseline}{Use an Open LLM as a Baseline}).
%The lack of transparency introduces challenges in analyzing mistakes, limiting the understanding of model failures.
%Open-source models offer greater transparency since the entire process is under the researcher's control; however, they require appropriate hardware.

Challenges with individual metrics include that, for example, \emph{BLEU-N} is a syntactic metric and hence does not measure semantic correctness or structural correctness.
Thus, a high \emph{BLEU-N} score does not directly indicate that the generated code is executable.
While alternatives exist, they often come with their own limitations.
For instance, \emph{Exact Match} is a strict measurement that does not account for functional equivalence but syntactically different code.
Execution-based metrics (e.g. pass@k) directly evaluate correctness by running test cases, but they require a setup with an execution environment.
When researchers observe unexpected values for certain metrics, the specific results should be investigated in more detail to uncover potential problems.
These problems can, for example, be related to formatting since code formatting highly influences metrics such as \emph{BLEU-N} or \emph{Exact Match}.

Another challenge to consider is that metrics usually capture one specific aspect of a task or solution.
For instance, metrics such as \emph{pass@k} do not reflect qualitative aspects of code such as maintainability, cognitive load, or readability.
These aspects are critical for the downstream task and influence the overall usability.
Moreover, benchmarks are isolated test sets and may not fully represent real-world applications.
For example, benchmarks such as \emph{HumanEval} synthesize code based on written specifications.
However, such explicit descriptions are rare in real-world applications.
Thus, evaluating the model performance with benchmarks might not reflect real-world tasks and end-user usability.

Finally, benchmark data contamination~\cite{DBLP:journals/corr/abs-2406-04244} continues to be a major challenge as well.
In many cases, the training data set for an LLM is not released in conjunction with the model.
The benchmark itself could be part of the model's training dataset.
Such benchmark contamination may lead to the model remembering the actual solution from the training data rather than solving the new task based on the seen data.
This leads to artificially high performance on the benchmark.
For unforeseen scenarios, however, the model might perform much worse.

%Models may become overfitted to the training data, leading to poor performance on unseen data.
%When optimizing for a metric, the underlying model might adjust its parameters to improve the metric, thereby failing to generalize.
%This can result in a model performing exceptionally well on a benchmark but struggling to hold up the performance in unforeseen settings, such as real-world scenarios.


\subsubsection{Study Types}

This guideline \must be followed for all study types that automatically evaluate the performance of LLMs or LLM-based tools.
The design of a benchmark and the selection of appropriate metrics are highly dependent on the specific study type and research goal.
Recommending specific metrics for specific study types is beyond the scope of these guidelines, but Cao et al. provide a good starting point and overview of existing metrics for evaluating LLMs~\cite{10.1145/3695988}.
Section \humanvalidation provides an overview of integrating human evaluation in LLM-based studies. 

For example, for \annotators, the research goal might be to assess which model comes close to a ground truth dataset created by human annotators.
Especially for open annotation tasks, selecting suitable metrics to compare LLM-generated and human-generated labels is important.
In general, annotation tasks can vary significantly: Are multiple labels allowed for the same sequence? Are the available labels predefined, or should the LLM generate a set of labels independently?
Due to this task dependence, researchers \must justify their metric choice, explaining what aspects of the task it captures and its limitations.

If researchers assess a well-established task, e.g. code generation, they \should report standard metrics pass@k and compare to other models.
If non-standard metrics are used they \must state the reasoning.
% What are characteristics of a specific study type?
% - 

% Benchmark
\subsubsection{References}

\bibliographystyle{plainnat}
\bibliography{../../literature.bib}

\end{document}


%% * [pass@k](https://arxiv.org/pdf/2107.03374) Pass@1 Pass@2 Pass@5 Pass@10, reporting of k depends on the specific usecase of the tool. e.g. for code completion in an IDE pass@1 is essential because the user only get to see one result, pass@5 and pass@10 show that LLMs are somewhat capable of solving the given task (indication that through e.g. fine-tuning it might be possible to solve) or if users get multiple options to choose from (e.g. test generation)
%% * [BLEU-N](https://aclanthology.org/P02-1040.pdf)
%% * [CodeBLEU](https://arxiv.org/abs/2009.10297)
%% * [CrystalBLEU](https://software-lab.org/publications/ase2022_CrystalBLEU.pdf)
%% * [ROUGE](https://aclanthology.org/W04-1013.pdf)
%% * Accuracy/Accuracy@k
%% * Mean Reciprocal Rank
%% * Edit Similarity (ES)
%% * Edit Distance (ED)
%% * Exact Match (EM)
%% * [METEOR](https://dl.acm.org/doi/pdf/10.5555/1626355.1626389)
%% * Recall
%% * F1-score
%% * Mean Reciprocal Rank (MRR)
%% * Mean Average Ranking (MAR)
%% * Mean First Ranking (MFR)
%% * [Character n-gram F-Score (ChrF)](https://aclanthology.org/W15-3049.pdf)
%% * [CodeBERTScore](https://arxiv.org/pdf/2302.05527)
%% * Perplexity (PP)


%% #### Domain-specific metrics
%%
%% * If a tool is analyzed, the acceptance rate of generated artifacts could be interesting (how many artifacts were accepted/rejected by the user)
%% * Inter-model-agreement (related to section on open LLM as baseline): Ask different LLMs or differently confidered LLMs and determine their agreement
%% * Cross Cutting Metrics: Costs/Energy requirements to get the results (database community often reports on how expensive it was to get results, would be interesting for LLMs too but nobody does it yet)

%% #### Classification

%% - Precision
%% - Recall
%% - F1-Score
%% - Accuracy
%% - Area Under the Receiver Operating Characteristic (ROC) Curve
%% - Receiver Operating Characteristic (ROC)
%% - False Positive Rate (FPR)
%% - False Negative Rate (FNR)
%% - Matthews Correlation Coefficient (MCC)

%% #### Recommendation

%% - Mean Reciprocal Rank (MRR)
%% - Precision/Precision@k
%% - MAP/MAP@k
%% - F-score/F-score@k
%% - Recall/Recall@k
%% - Accuracy

%% ### Benchmarks

%% ***TODO:*** Maybe something along the lines of using different benchmarks? Being aware of their biases (e.g., focus on a particular programming language such as Python)?

%% * [HumanEval](https://github.com/openai/human-eval)
%% * [REPOCOD](https://huggingface.co/datasets/lt-asset/REPOCOD)
%% * [ClassEval](https://arxiv.org/abs/2308.01861)

%% #### Code Translation

%% * [Avatar](https://arxiv.org/pdf/2108.11590)
%% * [Transcoder](https://arxiv.org/pdf/2006.03511)

% #### Baselines

% Provide a reference point for existing research
% - often include traditional non-LLM based
% - used to show a difference between (often) low-cost solutions and
% - should report the same metric e.g.
% Example:
% - LLM for test prioritisation
% - LLM
